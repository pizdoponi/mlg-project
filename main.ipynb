{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We used a subset of WaterDrop dataset from Deepmind. The videos only covers the specific case of a water droplet in vacuum, but that is fine with us, as that is exactly what we wanted to model!\n",
    "\n",
    "TODO: download the dataset and extract it in the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "1. Apply noise to the training to mitigate error accumulation over long rollouts. We use a simple approach to make the model more robust to noisy inputs: at training we corrupt the input velocities of the model with random-walk noise N (0, $\\sigma_v$ = 0.0003) (adjusting input positions), so the training distribution is closer to the distribution generated during rollouts. \n",
    "2. Normalize all input and target vectors elementwise to zero mean and unit variance, using statistics computed online during training. Preliminary experiments showed that normalization led to faster training, though converged performance was not noticeably improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "MLP is used in a lot of different places throughout the architecture, most notably the encoder and the decoder are both MLPs. We define it as a class to make it easier to use.\n",
    "\n",
    "All MLPs have two hidden layers (with ReLU activations), followed by a nonactivated output layer, each layer with size of 128. All MLPs (except the output decoder) are followed by a LayerNorm layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128, layer_norm=True):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.layer_norm = layer_norm\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        # The rationale behind setting the standard deviation of the normal distribution to 1/sqrt(layer.in_features)\n",
    "        # is to normalize the variance of the layer's inputs and outputs. This helps to prevent the outputs\n",
    "        # from exploding or vanishing during training. The 1/sqrt(layer.in_features) factor is based on the recommendation\n",
    "        # in the paper \"Understanding the difficulty of training deep feedforward neural networks\" by Glorot and Bengio (2010).\n",
    "        self.layer1.weight.data.normal_(0, 1 / torch.sqrt(self.layer1.in_features))\n",
    "        # Setting the bias to 0 allows the network to learn the appropriate bias values during training.\n",
    "        self.layer1.bias.data.fill_(0)\n",
    "        # The same reasoning applies to the other layers.\n",
    "        self.layer2.weight.data.normal_(0, 1 / torch.sqrt(self.layer2.in_features))\n",
    "        self.layer2.bias.data.fill_(0)\n",
    "        self.layer3.weight.data.normal_(0, 1 / torch.sqrt(self.layer3.in_features))\n",
    "        self.layer3.bias.data.fill_(0)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer2(x)\n",
    "        if self.layer_norm:\n",
    "            x = nn.LayerNorm(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
