{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset\n",
        "\n",
        "We used a subset of WaterDrop dataset from Deepmind. The videos only covers the specific case of a water droplet in vacuum, but that is fine with us, as that is exactly what we wanted to model!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Converting TFRecord to torch tensors\n",
        "\n",
        "Unfortunately, the dataset is not available in a format that is easy to use with PyTorch. We need to convert it to a format that is more suitable for PyTorch (e.g. `torch.Tensor`).\n",
        "\n",
        "We achieve this by iterating over the TFRecord file, extracting the features for each frame and adding this information to its corresponding trajctory. We then save the trajectories as a pickle file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "from read_dataset import prapare_data_from_tfds\n",
        "import torch\n",
        "\n",
        "# initialize dataset\n",
        "# this is of type tf.data.Dataset\n",
        "ds = prepare_data_from_tfds(data_path='dataset/WaterDrop/train.tfrecord', is_rollout=False, batch_size=1)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# kepp track of the number of batches we iterate over in the dataset\n",
        "batch_count = 0\n",
        "# there are many trajectories in the dataset, we want to save each trajectory in a separate folder\n",
        "number_of_trajectories = 0\n",
        "number_of_frames_in_trajectory = {}\n",
        "\n",
        "# this are the features we want to extract from the dataset\n",
        "positions = {}\n",
        "particle_type = {}\n",
        "labels = {}\n",
        "\n",
        "print('started loading data...')\n",
        "for features, current_labels in ds:\n",
        "    \n",
        "    # extract feature\n",
        "    number_of_particles = features['n_particles_per_example'][0]\n",
        "    current_positions = torch.tensor(features['position']).to(device)\n",
        "    current_labels = torch.tensor(current_labels).to(device)\n",
        "    \n",
        "    # if this is the first time we see this number of particles\n",
        "    # i.e. this is a new trajectory\n",
        "    if number_of_particles not in positions.keys():\n",
        "        number_of_trajectories += 1\n",
        "        number_of_frames_in_trajectory[number_of_particles] = 1\n",
        "        positions[number_of_particles] = current_positions\n",
        "        labels[number_of_particles] = current_labels\n",
        "        particle_type[number_of_particles] = torch.tensor(features['particle_type']).to(device)\n",
        "    \n",
        "    else:\n",
        "        # and if else is required here beacuse we need to stack the first two frames, and only than we can cat\n",
        "        if number_of_frames_in_trajectory[number_of_particles] == 1:\n",
        "            positions[number_of_particles] = torch.stack((positions[number_of_particles], current_positions), dim=0)\n",
        "            labels[number_of_particles] = torch.stack((labels[number_of_particles], current_labels), dim=0)\n",
        "            particle_type[number_of_particles] = torch.stack((particle_type[number_of_particles], torch.tensor(features['particle_type']).to(device)), dim=0)\n",
        "        \n",
        "        else:\n",
        "            positions[number_of_particles] = torch.cat((positions[number_of_particles], current_positions.unsqueeze(0)), dim=0)\n",
        "            labels[number_of_particles] = torch.cat((labels[number_of_particles], current_labels.unsqueeze(0)), dim=0)\n",
        "            particle_type[number_of_particles] = torch.cat((particle_type[number_of_particles], torch.tensor(features['particle_type']).to(device).unsqueeze(0)), dim=0)\n",
        "            \n",
        "        number_of_frames_in_trajectory[number_of_particles] += 1\n",
        "    \n",
        "    print(f'batch {batch_count} done; number of trajectories: {number_of_trajectories}')\n",
        "    batch_count += 1\n",
        "    \n",
        "    # the main way to end the loop\n",
        "    # change this if you want a different number of trajectories\n",
        "    # note: trajectories are randomly mixes and do not appear in the\n",
        "    if number_of_trajectories == 22:\n",
        "        break\n",
        "    \n",
        "print('all batches done')\n",
        "print('converting to tensors...')\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "trajectory_dir_count = 0\n",
        "for trajectory_dir_count, n_particles in enumerate(positions.keys()):\n",
        "    if os.path.exists(f'data/trajectories/{trajectory_dir_count}'):\n",
        "        shutil.rmtree(f'data/trajectories/{trajectory_dir_count}')\n",
        "        print(f'data/trajectories/{trajectory_dir_count} deleted')\n",
        "        \n",
        "    os.makedirs(f'data/trajectories/{trajectory_dir_count}')\n",
        "    print(f'data/trajectories/{trajectory_dir_count} created')\n",
        "    \n",
        "    with open(f'data/trajectories/{trajectory_dir_count}/positions.pt', 'wb') as f:\n",
        "        torch.save(positions[n_particles], f)\n",
        "        print(f'data/trajectories/{trajectory_dir_count}/positions.pt saved')\n",
        "        \n",
        "    with open(f'data/trajectories/{trajectory_dir_count}/labels.pt', 'wb') as f:\n",
        "        torch.save(labels[n_particles], f)\n",
        "        print(f'data/trajectories/{trajectory_dir_count}/labels.pt saved')\n",
        "        \n",
        "    with open(f'data/trajectories/{trajectory_dir_count}/particle_type.pt', 'wb') as f:\n",
        "        torch.save(particle_type[n_particles], f)\n",
        "        print(f'data/trajectories/{trajectory_dir_count}/particle_type.pt saved')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sorting the frames\n",
        "Below is the function that calculates the permutation that sorts the data by the frame number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# sort frames\n",
        "def get_sorted_data(data):\n",
        "    '''\n",
        "    data: numpy array: (num_of_frames, number_of_particles, dimension)\n",
        "    return: numpy (num_of_frames, number_of_particles, dimension), with sorted frames\n",
        "    '''\n",
        "    \n",
        "    # class for groups representation: arrays of frames\n",
        "    class ObjectGroup:\n",
        "        def __init__(self, elems):\n",
        "            self.elems = elems\n",
        "        \n",
        "        def add_to_begining(self, elem):\n",
        "            self.elems.insert(0, elem)\n",
        "        \n",
        "        def add_to_end(self, elem):\n",
        "            self.elems.append(elem)\n",
        "\n",
        "        def reverse(self):\n",
        "            self.elems.reverse()\n",
        "\n",
        "    # merge\n",
        "    def merge_groups(g1, g2):\n",
        "        lst = g1.elems\n",
        "        lst.extend(g2.elems)\n",
        "        return ObjectGroup(lst)\n",
        "\n",
        "    # min distance between groups: potentially can be merged front and back \n",
        "    def get_difference(group1, group2):\n",
        "        dist1 = d_matrix[group1.elems[0], group2.elems[0]]\n",
        "        dist2 = d_matrix[group1.elems[0], group2.elems[-1]]\n",
        "        dist3 = d_matrix[group1.elems[-1], group2.elems[0]]\n",
        "        dist4 = d_matrix[group1.elems[-1], group2.elems[-1]]\n",
        "        return min(dist1, dist2, dist3, dist4)\n",
        "\n",
        "    # get index: for flipping\n",
        "    def get_ind(group1, group2):\n",
        "        d = get_difference(group1, group2)\n",
        "        if d == d_matrix[group1.elems[0], group2.elems[0]]:\n",
        "            return 1\n",
        "        if d == d_matrix[group1.elems[0], group2.elems[-1]]:\n",
        "            return 2\n",
        "        if d == d_matrix[group1.elems[-1], group2.elems[0]]:\n",
        "            return 3\n",
        "        if d == d_matrix[group1.elems[-1], group2.elems[-1]]:\n",
        "            return 4\n",
        "        return 4\n",
        "    \n",
        "    # define distance\n",
        "    def get_distance(points_1, points_2):\n",
        "        return np.sum(np.abs(points_1-points_2)**2)**(1./2)\n",
        "\n",
        "    # calculate distances\n",
        "    def calculate_distance_matrix(points):\n",
        "        d_matrix = np.zeros((len(points), len(points)))\n",
        "        for i in range(len(points)):\n",
        "            for j in range(len(points)):\n",
        "                d_matrix[i,j] = get_distance(points[i], points[j])\n",
        "                d_matrix[j,i] = d_matrix[i,j]\n",
        "        #print(np.max(d_matrix))\n",
        "        return d_matrix\n",
        "\n",
        "    # objects:\n",
        "    obj_list = []\n",
        "    p = []\n",
        "    for i in range(data.shape[0]):\n",
        "        oo = ObjectGroup([i])\n",
        "        obj_list.append(oo)\n",
        "        p.append(data[i])\n",
        "\n",
        "    d_matrix = calculate_distance_matrix(p)\n",
        "\n",
        "    # at each iteration we merge sequences until one remains\n",
        "    for i in range(data.shape[0]-1):\n",
        "        ind_1 = -1\n",
        "        ind_2 = -1\n",
        "        min_dist = np.inf\n",
        "        for j in range(len(obj_list)):\n",
        "            for k in range(j+1, len(obj_list)):\n",
        "                dist = get_difference(obj_list[j], obj_list[k])\n",
        "                if dist < min_dist:\n",
        "                    min_dist = dist\n",
        "                    ind_1 = j\n",
        "                    ind_2 = k\n",
        "        which_d = get_ind(obj_list[ind_1], obj_list[ind_2])\n",
        "        \n",
        "        # which lists flip\n",
        "        if which_d == 1 or which_d == 2:\n",
        "            obj_list[ind_1].reverse()\n",
        "        if which_d == 2 or which_d == 4:\n",
        "            obj_list[ind_2].reverse()\n",
        "        obj_1 = obj_list[ind_1]\n",
        "        obj_2 = obj_list[ind_2]\n",
        "        merged = merge_groups(obj_1, obj_2)\n",
        "\n",
        "        # update obj_list\n",
        "        obj_list.remove(obj_1)\n",
        "        obj_list.remove(obj_2)\n",
        "        obj_list.append(merged)\n",
        "    \n",
        "    #print(obj_list[0].elems)\n",
        "    l = obj_list[0].elems\n",
        "    #sorted_d = data[obj_list[0].elems]\n",
        "    if get_distance(data[l[0]], data[l[1]]) > get_distance(data[l[-1]], data[l[-2]]):\n",
        "        l = l[::-1]\n",
        "    \n",
        "    return data[l], l"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have the permutations, we can sort the data. For convenience, we have included the permutation indexes for the first 20 trajectories. This way, you don't have to wait for the sorting to finish. You can find them in `permutations.py`. Each permutation is a list of integers, where the index of the integer is the frame number and the value is the index of the frame in the original trajectory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from permutations import *\n",
        "import os\n",
        "\n",
        "os.makedirs('./data/train', exist_ok=True)\n",
        "os.makedirs('./data/valid', exist_ok=True)\n",
        "\n",
        "# for each of the train, valid, test specify what trajectories to use\n",
        "train_indexes = [0,1,2]\n",
        "valid_indexes = [3,4]\n",
        "test_indexes = [5,6]\n",
        "\n",
        "indexes = [index_0, index_1, index_2, index_3, index_4, index_5, index_6]\n",
        "\n",
        "for train_index in train_indexes:\n",
        "    positions = torch.load('./data/trajectories/{}/positions.pt'.format(train_index))\n",
        "    labels = torch.load('./data/trajectories/{}/labels.pt'.format(train_index))\n",
        "    particle_type = torch.load('./data/trajectories/{}/particle_type.pt'.format(train_index))\n",
        "    positions = positions[indexes[train_index]]\n",
        "    labels = labels[indexes[train_index]]\n",
        "    particle_type = particle_type[indexes[train_index]]\n",
        "    os.makedirs('./data/train/{}'.format(train_index), exist_ok=True)\n",
        "    with open('./data/train/{}/positions.pt'.format(train_index), 'wb') as f:\n",
        "        torch.save(positions, f)\n",
        "    with open('./data/train/{}/labels.pt'.format(train_index), 'wb') as f:\n",
        "        torch.save(labels, f)\n",
        "    with open('./data/train/{}/particle_type.pt'.format(train_index), 'wb') as f:\n",
        "        torch.save(particle_type, f)\n",
        "\n",
        "for valid_index in valid_indexes:\n",
        "    positions = torch.load('./data/trajectories/{}/positions.pt'.format(valid_index))\n",
        "    labels = torch.load('./data/trajectories/{}/labels.pt'.format(valid_index))\n",
        "    particle_type = torch.load('./data/trajectories/{}/particle_type.pt'.format(valid_index))\n",
        "    positions = positions[indexes[valid_index]]\n",
        "    labels = labels[indexes[valid_index]]\n",
        "    particle_type = particle_type[indexes[valid_index]]\n",
        "    os.makedirs('./data/valid/{}'.format(valid_index), exist_ok=True)\n",
        "    with open('./data/valid/{}/positions.pt'.format(valid_index), 'wb') as f:\n",
        "        torch.save(positions, f)\n",
        "    with open('./data/valid/{}/labels.pt'.format(valid_index), 'wb') as f:\n",
        "        torch.save(labels, f)\n",
        "    with open('./data/valid/{}/particle_type.pt'.format(valid_index), 'wb') as f:\n",
        "        torch.save(particle_type, f)\n",
        "\n",
        "for test_index in test_indexes:\n",
        "    positions = torch.load('./data/trajectories/{}/positions.pt'.format(test_index))\n",
        "    labels = torch.load('./data/trajectories/{}/labels.pt'.format(test_index))\n",
        "    particle_type = torch.load('./data/trajectories/{}/particle_type.pt'.format(test_index))\n",
        "    positions = positions[indexes[test_index]]\n",
        "    labels = labels[indexes[test_index]]\n",
        "    particle_type = particle_type[indexes[test_index]]\n",
        "    os.makedirs('./data/test/{}'.format(test_index), exist_ok=True)\n",
        "    with open('./data/test/{}/positions.pt'.format(test_index), 'wb') as f:\n",
        "        torch.save(positions, f)\n",
        "    with open('./data/test/{}/labels.pt'.format(test_index), 'wb') as f:\n",
        "        torch.save(labels, f)\n",
        "    with open('./data/test/{}/particle_type.pt'.format(test_index), 'wb') as f:\n",
        "        torch.save(particle_type, f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting the positions\n",
        "\n",
        "The last step of our data preparation step is to extract the positions. In the dataset, for each frame and for every particle, there is a window of size 6, which contains the information about the previous positions. We want to extract it in such a way, that each position tensor will have a shape of size `(<number of frames = 1000>, <number of particles>, <dimension> = 2)`. This way we have the complete freedom to choose the window size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch_geometric as pyg\n",
        "\n",
        "def generate_noise(position_seq, noise_std=3e-4):\n",
        "    \"\"\"Generate noise for a trajectory\"\"\"\n",
        "    velocity_seq = position_seq[:, 1:] - position_seq[:, :-1]\n",
        "    time_steps = velocity_seq.size(1)\n",
        "    velocity_noise = torch.randn_like(velocity_seq) * (noise_std / time_steps ** 0.5)\n",
        "\n",
        "    # restrict possible values: |velocity_noise[i,j,k]| <= (noise_std / time_steps ** 0.5)\n",
        "    velocity_noise = torch.minimum(velocity_noise, torch.ones(velocity_noise.shape) * (noise_std / time_steps ** 0.5))\n",
        "    velocity_noise = torch.maximum(velocity_noise, -torch.ones(velocity_noise.shape) * (noise_std / time_steps ** 0.5))\n",
        "    \n",
        "    velocity_noise = velocity_noise.cumsum(dim=1)\n",
        "    position_noise = velocity_noise.cumsum(dim=1)\n",
        "    position_noise = torch.cat((torch.zeros_like(position_noise)[:, 0:1], position_noise), dim=1)\n",
        "    return position_noise\n",
        "\n",
        "\n",
        "def preprocess(particle_type, position_seq, target_position, metadata, noise_std=3e-4):\n",
        "    \"\"\"Preprocess a trajectory and construct the graph\"\"\"\n",
        "    # apply noise to the trajectory\n",
        "    position_noise = generate_noise(position_seq, noise_std)\n",
        "    position_seq = position_seq + position_noise\n",
        "\n",
        "    # calculate the velocities of particles\n",
        "    recent_position = position_seq[:, -1]\n",
        "    velocity_seq = position_seq[:, 1:] - position_seq[:, :-1]\n",
        "\n",
        "    # construct the graph based on the distances between particles\n",
        "    n_particle = recent_position.size(0)\n",
        "    edge_index = pyg.nn.radius_graph(recent_position, metadata[\"default_connectivity_radius\"], loop=True, max_num_neighbors=n_particle)\n",
        "    \n",
        "    # node-level features: velocity, distance to the boundary\n",
        "    normal_velocity_seq = (velocity_seq - torch.tensor(metadata[\"vel_mean\"])) / torch.sqrt(torch.tensor(metadata[\"vel_std\"]) ** 2 + noise_std ** 2)\n",
        "    boundary = torch.tensor(metadata[\"bounds\"])\n",
        "    distance_to_lower_boundary = recent_position - boundary[:, 0]\n",
        "    distance_to_upper_boundary = boundary[:, 1] - recent_position\n",
        "    distance_to_boundary = torch.cat((distance_to_lower_boundary, distance_to_upper_boundary), dim=-1)\n",
        "    distance_to_boundary = torch.clip(distance_to_boundary / metadata[\"default_connectivity_radius\"], -1.0, 1.0)\n",
        "\n",
        "    # edge-level features: displacement, distance\n",
        "    dim = recent_position.size(-1)\n",
        "    edge_displacement = (torch.gather(recent_position, dim=0, index=edge_index[0].unsqueeze(-1).expand(-1, dim)) -\n",
        "                   torch.gather(recent_position, dim=0, index=edge_index[1].unsqueeze(-1).expand(-1, dim)))\n",
        "    edge_displacement /= metadata[\"default_connectivity_radius\"]\n",
        "    edge_distance = torch.norm(edge_displacement, dim=-1, keepdim=True)\n",
        "\n",
        "    # ground truth for training\n",
        "    if target_position is not None:\n",
        "        last_velocity = velocity_seq[:, -1]\n",
        "        next_velocity = target_position + position_noise[:, -1] - recent_position\n",
        "        acceleration = next_velocity - last_velocity\n",
        "        acceleration = (acceleration - torch.tensor(metadata[\"acc_mean\"])) / torch.sqrt(torch.tensor(metadata[\"acc_std\"]) ** 2 + noise_std ** 2)\n",
        "    else:\n",
        "        acceleration = None\n",
        "\n",
        "    # return the graph with features\n",
        "    graph = pyg.data.Data(\n",
        "        x=particle_type,\n",
        "        edge_index=edge_index,\n",
        "        edge_attr=torch.cat((edge_displacement, edge_distance), dim=-1),\n",
        "        y=acceleration,\n",
        "        pos=torch.cat((velocity_seq.reshape(velocity_seq.size(0), -1), distance_to_boundary), dim=-1)\n",
        "    )\n",
        "    return graph"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset object\n",
        "\n",
        "We create a custom dataset object that will be used to load the data. The dataset object will be used by the dataloader to load the data in batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from torch import load, cat, from_numpy\n",
        "from os.path import join\n",
        "from os import listdir\n",
        "from torch_geometric.data import Dataset\n",
        "\n",
        "class OneStepDataset(Dataset):\n",
        "    def __init__(self, path_metadata, dir_trajectories, split=\"\", window_length=7, noise=0.0, return_pos=False) -> None:\n",
        "        super().__init__()\n",
        "        \"\"\"Creates a dataset for one step predictions, using noise. Also sorts the positions to \n",
        "        correct order.\n",
        "        Args:\n",
        "            path_metadata (str): path of directory where metadata for dataset is stored\n",
        "            dir_trajectories (str): path of directory with ONLY directories of trajectories, \n",
        "                which contain corresponding data\n",
        "            split (str, Optional): not yet implemented, currently not in use\n",
        "            window_length (int, Optional): size of window that are looked in parallel when creating a graph\n",
        "            noise (int, Optional): standard deviation that will be used to generate noise to position data\n",
        "            return_pos (bool, Optional): if `True` it returns the last positions in a sequence besides the graph\n",
        "        \"\"\"\n",
        "        with open(join(path_metadata, \"metadata.json\")) as f:\n",
        "            self.metadata = json.load(f)\n",
        "\n",
        "        # helper variable of all dirs containing data for each trajectory\n",
        "        traj_dir = listdir(dir_trajectories)\n",
        "\n",
        "        # particle type tensor for each trajectory, shape = (num_traj, num_particles)\n",
        "        self.particle_type = [\n",
        "            load(join(dir_trajectories, i, 'particle_type.pt')) for i in traj_dir\n",
        "        ]\n",
        "\n",
        "        # particle position tensor for each trajectory, shape = (num_traj, num_frames, num_particles, num_dim)\n",
        "        # ! POSITIONS IN THE TRAIN AND VALID FOLDER ARE ALREADY SORTED\n",
        "        _sorted = True\n",
        "        if not _sorted:\n",
        "            self.positions = [\n",
        "                from_numpy(\n",
        "                    get_sorted_data(\n",
        "                        load(join(dir_trajectories, i, 'positions.pt')).detach().numpy())[0]\n",
        "                )\n",
        "                for i in traj_dir\n",
        "            ]\n",
        "        else:\n",
        "            self.positions = [load(join(dir_trajectories, i, 'positions.pt')) for i in traj_dir]\n",
        "\n",
        "        self.window_length = window_length\n",
        "        self.noise = noise\n",
        "        self.return_pos = return_pos\n",
        "        self.dim = self.positions[0].shape[-1]\n",
        "\n",
        "        # cut trajectories according to time slices\n",
        "        self.windows = []\n",
        "        for i, traj in enumerate(self.positions):\n",
        "            size = traj.shape[1]\n",
        "            length = traj.shape[0] - window_length + 1\n",
        "            for k in range(length):\n",
        "                desc = {\n",
        "                    \"size\": size,\n",
        "                    \"pos\": (i, k),\n",
        "                }\n",
        "                self.windows.append(desc)\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.windows)\n",
        "    \n",
        "    def get(self, idx):\n",
        "        window = self.windows[idx]\n",
        "        size = window[\"size\"]\n",
        "        traj, frame = window[\"pos\"]\n",
        "\n",
        "        particle_type = self.particle_type[traj][0].detach().clone()\n",
        "        position_seq = self.positions[traj][frame:frame + self.window_length].detach().clone()\n",
        "        position_seq = position_seq.permute(1, 0, 2)\n",
        "        target_position = position_seq[:, -1]\n",
        "        position_seq = position_seq[:, :-1]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            graph = preprocess(particle_type, position_seq, target_position, self.metadata, self.noise)\n",
        "        if self.return_pos:\n",
        "          return graph, position_seq[:, -1]\n",
        "        return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "data_types = ('train', 'valid', 'test')\n",
        "for data_type in data_types:\n",
        "    trajectory_path = os.path.join('data', data_type)\n",
        "    for trajectory_i in os.listdir(trajectory_path):\n",
        "        position_seq = torch.load(os.path.join(trajectory_path, trajectory_i, 'positions.pt'))\n",
        "        if len(position_seq.shape) == 3:\n",
        "            continue\n",
        "        # from the first frame (dimension 0)\n",
        "        # for all of the particles (dimension 1)\n",
        "        # get the first 5 positions (dimension 2)\n",
        "        # and the x and y coordinates (dimension 3)\n",
        "        # and extract them in a tensor of shape (5, num_particles, 2)\n",
        "        first_5_positions = position_seq[0, :, :5, :]\n",
        "        first_5_positions = first_5_positions.permute(1, 0, 2)\n",
        "        # for all the other frames\n",
        "        # extract only the last position\n",
        "        # and join them to the first 5 positions\n",
        "        # to get the final shape of (num_frames = 1000, num_particles, 2)\n",
        "        last_positions = position_seq[:, :, -1, :]\n",
        "        final_positions = torch.cat((first_5_positions, last_positions), dim=0)\n",
        "        # save the final positions\n",
        "        with open(os.path.join(trajectory_path, trajectory_i, 'positions.pt'), 'wb') as f:\n",
        "            print(f'fixed {trajectory_i}')\n",
        "            torch.save(final_positions, f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GNN model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MLP\n",
        "\n",
        "MLP is used in a lot of different places throughout the architecture, most notably the encoder and the decoder are both MLPs. We define it as a class to make it easier to use.\n",
        "\n",
        "All MLPs have two hidden layers (with ReLU activations), followed by a nonactivated output layer, each layer with size of 128. All MLPs (except the output decoder) are followed by a LayerNorm layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, output_dim, hidden_dim=128, layer_norm=True):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.layer3 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.layerNorm = nn.LayerNorm(output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer_norm = layer_norm\n",
        "    \n",
        "    def reset_parameters(self):\n",
        "        # The rationale behind setting the standard deviation of the normal distribution to 1/sqrt(layer.in_features)\n",
        "        # is to normalize the variance of the layer's inputs and outputs. This helps to prevent the outputs\n",
        "        # from exploding or vanishing during training. The 1/sqrt(layer.in_features) factor is based on the recommendation\n",
        "        # in the paper \"Understanding the difficulty of training deep feedforward neural networks\" by Glorot and Bengio (2010).\n",
        "        self.layer1.weight.data.normal_(0, 1 / torch.sqrt(self.layer1.in_features))\n",
        "        # Setting the bias to 0 allows the network to learn the appropriate bias values during training.\n",
        "        self.layer1.bias.data.fill_(0)\n",
        "        # The same reasoning applies to the other layers.\n",
        "        self.layer2.weight.data.normal_(0, 1 / torch.sqrt(self.layer2.in_features))\n",
        "        self.layer2.bias.data.fill_(0)\n",
        "        self.layer3.weight.data.normal_(0, 1 / torch.sqrt(self.layer3.in_features))\n",
        "        self.layer3.bias.data.fill_(0)\n",
        "        \n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.relu(x)\n",
        "        if self.layer_norm:\n",
        "            x = self.layerNorm(x)\n",
        "        return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GNN Layer\n",
        "Here we implement InteractionNetwork\\\n",
        "paper: https://proceedings.neurips.cc/paper_files/paper/2016/file/3147da8ab4a0437c15ef51a5cc7f2dc4-Paper.pdf\n",
        "\n",
        "We use MPL that we defined above to generate messages for nodes and edges.\n",
        "\n",
        "Updaed features for nodes, `v_i` and edges, `e_ij`:\n",
        "\n",
        "$$\n",
        "v_i^{k+1} = v_i^k + MLP_n(v_i^k, \\sum_{v_j \\in N(v_i)}{MPL_e(v_i^k, v_j^k, e_ij^k)}) \\\\\n",
        "$$ \n",
        "$$ e_{ij}^{k+1} = e_{ij}^k + MLP_e(v_i^k, v_j^k, e_{ij}^k) $$\n",
        "\n",
        "where $MLP_e(\\cdot, \\cdot, \\cdot)$ is only computed once and then used twice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_scatter import scatter\n",
        "from torch import cat\n",
        "\n",
        "class InteractionNetwork(MessagePassing):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.node_msg = MLP(2 * hidden_dim, hidden_dim, hidden_dim)\n",
        "        self.edge_msg = MLP(3 * hidden_dim, hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_feature):\n",
        "        # propagate invokes message() and aggregate(), which return (inputs, out)\n",
        "        # we update edge feature as: current edge feature + current message passing it\n",
        "        edge_out, aggr = self.propagate(edge_index, x=(x, x), edge_feature=edge_feature)\n",
        "        edge_out = edge_feature + edge_out\n",
        "\n",
        "        # we update node features as: sum of neigbouring messages and current\n",
        "        # node feature get passed through coresponding MLP.\n",
        "        # To include self correction a bit we add current feature to that output\n",
        "        node_out = x + self.node_msg(cat((x, aggr), dim=-1))\n",
        "\n",
        "        return node_out, edge_out\n",
        "\n",
        "    def message(self, x_i, x_j, edge_feature):\n",
        "        # here we create messages as an output of MPL with 3 inputs:\n",
        "        # edge feature and feature of each node connected by this edge\n",
        "        x = self.edge_msg(cat((x_i, x_j, edge_feature), dim=-1))\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def aggregate(self, source, index, dim_size=None):\n",
        "        # we sum all neighbouring messages for each node, which we will use to \n",
        "        # update next layer of node features\n",
        "        out = scatter(source, index, dim_size=dim_size, dim=self.node_dim, reduce=\"sum\")\n",
        "\n",
        "        return (source, out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.nn import Embedding, ModuleList\n",
        "\n",
        "class GNS(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_dim = 128,\n",
        "        num_types = 9,\n",
        "        emb_dim = 16,\n",
        "        num_gnn_layers = 5,\n",
        "        simulation_dim = 2\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # IMPORTANT: this is the input dimension of data. It means that the model\n",
        "        # gets data from 2 previouos frames(2*sim_dim) plus the embedding.\n",
        "        # this variable is precomputed here for transparency and used in node_input\n",
        "        # new: hardcoded -> 14 = 7 * 2, for each frame(7) we have pos and vel (1+1)\n",
        "        node_input_dim = 14 + emb_dim\n",
        "        \n",
        "        # classic torch.nn Embedding\n",
        "        self.embedding = Embedding(num_types, emb_dim)\n",
        "\n",
        "        # node inputs and outputs\n",
        "        self.node_input = MLP(node_input_dim, hidden_dim, hidden_dim)\n",
        "        self.node_output = MLP(hidden_dim, simulation_dim, hidden_dim)\n",
        "\n",
        "        self.edge_input = MLP(simulation_dim + 1, hidden_dim, hidden_dim, layer_norm=False)\n",
        "\n",
        "        # initialize gnn layers as InteractionNetwork layers\n",
        "        self.gnns = ModuleList([InteractionNetwork(hidden_dim) for i in range(num_gnn_layers)])\n",
        "\n",
        "        # just save number of layers for later use\n",
        "        self.num_gnns = num_gnn_layers\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
        "\n",
        "    def forward(self, data):\n",
        "        # first we embed the data into features\n",
        "        # SELF NOTE: I was guessing that `data` will have keys `x`, `pos`, `edge_attr` and `edge_index`\n",
        "        \n",
        "        #print(cat((self.embedding(data.x)[:,0,0], data.pos[:,0]), dim=-1).shape)\n",
        "        #node_features = self.node_input(cat((self.embedding(data.x), data.pos), dim=-1))\n",
        "        node_features = cat((self.embedding(data.x), data.pos), dim=-1)\n",
        "        \n",
        "        node_features = self.node_input(node_features)\n",
        "        edge_features = self.edge_input(data.edge_attr)\n",
        "\n",
        "        # then propagate them trough model layers\n",
        "        for gnn in self.gnns:\n",
        "            node_features, edge_features = gnn(x=node_features, edge_feature=edge_features, edge_index=data.edge_index)\n",
        "\n",
        "        # and finally return node positions, in our case: x, y coordinates\n",
        "        node_output = self.node_output(node_features)\n",
        "\n",
        "        return node_output"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and testing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first define the arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = {\n",
        "        'input_dim': 3,\n",
        "        'output_dim': 3,\n",
        "        'hidden_dim': 128,\n",
        "        'layer_norm': True,\n",
        "        'lr': 0.01,\n",
        "        'weight_decay': 5e-3,\n",
        "        'batch_size': 1,\n",
        "        'epochs': 1,\n",
        "        'dropout': 0.5,\n",
        "        'opt': 'adam',\n",
        "        'validate_interval': 1000,\n",
        "        'save_model': True,\n",
        "        'model_path': './model.pt'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch_geometric as pyg\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_onestep(model: torch.nn.Module, data_loader: pyg.data.Dataset, device: torch.device): # type: ignore\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for batch_number, data in enumerate(data_loader):\n",
        "        data = data.to(device)\n",
        "        out = model(data)\n",
        "        loss = F.mse_loss(out, data.y)\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / (batch_number + 1) # reportUnboundVariable: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def train(args: dict[str, Any], model: torch.nn.Module, train_loader, valid_loader):    \n",
        "    # Set the device to GPU if available, otherwise CPU\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "    \n",
        "    # init optimiser\n",
        "    if args['opt'] == 'adam':\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
        "    elif args['opt'] == 'sgd':\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
        "    else:\n",
        "        raise ValueError('Unknown optimizer: {}'.format(args['opt']))\n",
        "    \n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
        "    \n",
        "    # loss function\n",
        "    loss_fn = nn.MSELoss()\n",
        "    \n",
        "    # track the losses to be able to plot the learning curve\n",
        "    train_loss = []\n",
        "    validate_loss = []\n",
        "    \n",
        "    # track the total number of steps\n",
        "    steps = 0\n",
        "    \n",
        "    # main train loop\n",
        "    i = 0\n",
        "    for epoch in range(args['epochs']):\n",
        "        i += 1\n",
        "        print(i)\n",
        "        model.train()\n",
        "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
        "        \n",
        "        # keep track of the total loss and the number of batches\n",
        "        total_loss = 0\n",
        "        batch_count = 0\n",
        "        \n",
        "        for data in progress_bar:\n",
        "            # forward pass\n",
        "            optimizer.zero_grad()\n",
        "            data = data.to(device)\n",
        "            model = model.to(device)\n",
        "            out = model(data)\n",
        "            loss = loss_fn(out, data.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            \n",
        "            # update progress bar\n",
        "            total_loss += loss.item()\n",
        "            batch_count += 1\n",
        "            progress_bar.set_postfix({\"loss\": loss.item(), \"avg_loss\": total_loss / batch_count})\n",
        "            steps += 1\n",
        "            train_loss.append((steps, loss.item()))\n",
        "\n",
        "            # evaluation\n",
        "            if steps % args[\"validate_interval\"] == 0:\n",
        "                model.eval()\n",
        "                loss = validate_onestep(model, valid_loader, device)\n",
        "                validate_loss.append((steps, loss))\n",
        "                tqdm.write(f\"\\nEval: Loss: {validate_loss}\")\n",
        "                model.train()\n",
        "    \n",
        "    if args['save_model']:\n",
        "        torch.save(model.state_dict(), args['model_path'])\n",
        "    \n",
        "    return train_loss, validate_loss"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now for the actual initialization of the data, model and training of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# here the actual training takes place\n",
        "train_dataset = OneStepDataset(\"dataset/WaterDropSample/\", \"data/train\") # TODO: init train dataset\n",
        "valid_dataset = OneStepDataset(\"dataset/WaterDropSample/\", \"data/valid\") # TODO: init valid dataset\n",
        "\n",
        "# train_loader = pyg.data.DataLoader(train_dataset, batch_size=args['batch_size'], drop_last=True, shuffle=True, pin_memory=True, num_workers=8)\n",
        "# valid_loader = pyg.data.DataLoader(valid_dataset, batch_size=args['batch_size'], drop_last=True, shuffle=False, pin_memory=True, num_workers=8)\n",
        "\n",
        "train_loader = pyg.data.DataLoader(train_dataset, batch_size=args['batch_size'], drop_last=True, shuffle=False)\n",
        "valid_loader = pyg.data.DataLoader(valid_dataset, batch_size=args['batch_size'], drop_last=True, shuffle=False)\n",
        "\n",
        "#train_loader.dataset.get(0)\n",
        "\n",
        "model = GNS() # TODO: init model to GNS\n",
        "\n",
        "train_loss, validate_loss = train(args, model, train_loader, valid_loader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot the loss curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# visualize the loss curve\n",
        "plt.figure()\n",
        "plt.plot(*zip(*train_loss), label=\"train\")\n",
        "plt.plot(*zip(*validate_loss), label=\"valid\")\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
