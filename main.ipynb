{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We used a subset of WaterDrop dataset from Deepmind. The videos only covers the specific case of a water droplet in vacuum, but that is fine with us, as that is exactly what we wanted to model!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting TFRecord to torch tensors\n",
    "\n",
    "Unfortunately, the dataset is not available in a format that is easy to use with PyTorch. We need to convert it to a format that is more suitable for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # for example in tf.data.TFRecordDataset(\"./dataset/WaterDropSample/test.tfrecord\").take(1):\n",
    "# #     parsed_example = tf.train.Example.FromString(example.numpy())\n",
    "# #     print(parsed_example.features.feature)\n",
    "\n",
    "# raw_dataset = tf.data.TFRecordDataset(\"./dataset/WaterDropSample/test.tfrecord\")\n",
    "\n",
    "# for raw_record in raw_dataset.take(1):\n",
    "#     example = tf.train.Example()\n",
    "#     example.ParseFromString(raw_record.numpy())\n",
    "#     # print(example)\n",
    "    \n",
    "#     result = {}\n",
    "#     # example.features.feature is the dictionary\n",
    "#     for key, feature in example.features.feature.items():\n",
    "#     # The values are the Feature objects which contain a `kind` which contains:\n",
    "#     # one of three fields: bytes_list, float_list, int64_list\n",
    "\n",
    "#         kind = feature.WhichOneof('kind')\n",
    "#         #print(kind)\n",
    "#         result[key] = np.array(getattr(feature, kind).value)\n",
    "#         #print(result[key])\n",
    "#         #print(result[key].dtype.type)\n",
    "\n",
    "#         # exmaple: particle_type: bytes_list -> numpy array unit8 (= byte array)\n",
    "#         # looks like we don't need conversion of float_list and int64_list types (not proven)\n",
    "#         if result[key].dtype.type == np.bytes_:\n",
    "#             arr = np.frombuffer(b''.join(result[key]), dtype=np.uint8)\n",
    "#             arr = arr.reshape((-1, len(result[key][0])))\n",
    "#             result[key] = arr\n",
    "#             #print(\"akka\")\n",
    "\n",
    "#     print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "1. Apply noise to the training to mitigate error accumulation over long rollouts. We use a simple approach to make the model more robust to noisy inputs: at training we corrupt the input velocities of the model with random-walk noise N (0, $\\sigma_v$ = 0.0003) (adjusting input positions), so the training distribution is closer to the distribution generated during rollouts. \n",
    "2. Normalize all input and target vectors elementwise to zero mean and unit variance, using statistics computed online during training. Preliminary experiments showed that normalization led to faster training, though converged performance was not noticeably improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rok/Documents/uni/mlg/mlg-project/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "\n",
    "def generate_noise(position_seq, noise_std=3e-4):\n",
    "    \"\"\"Generate noise for a trajectory\"\"\"\n",
    "    velocity_seq = position_seq[:, 1:] - position_seq[:, :-1]\n",
    "    time_steps = velocity_seq.size(1)\n",
    "    velocity_noise = torch.randn_like(velocity_seq) * (noise_std / time_steps ** 0.5)\n",
    "\n",
    "    # restrict possible values: |velocity_noise[i,j,k]| <= (noise_std / time_steps ** 0.5)\n",
    "    velocity_noise = torch.minimum(velocity_noise, torch.ones(velocity_noise.shape) * (noise_std / time_steps ** 0.5))\n",
    "    velocity_noise = torch.maximum(velocity_noise, -torch.ones(velocity_noise.shape) * (noise_std / time_steps ** 0.5))\n",
    "    \n",
    "    velocity_noise = velocity_noise.cumsum(dim=1)\n",
    "    position_noise = velocity_noise.cumsum(dim=1)\n",
    "    position_noise = torch.cat((torch.zeros_like(position_noise)[:, 0:1], position_noise), dim=1)\n",
    "    return position_noise\n",
    "\n",
    "\n",
    "def preprocess(particle_type, position_seq, target_position, metadata, noise_std=3e-4):\n",
    "    \"\"\"Preprocess a trajectory and construct the graph\"\"\"\n",
    "    # apply noise to the trajectory\n",
    "    position_noise = generate_noise(position_seq, noise_std)\n",
    "    position_seq = position_seq + position_noise\n",
    "\n",
    "    # calculate the velocities of particles\n",
    "    recent_position = position_seq[:, -1]\n",
    "    velocity_seq = position_seq[:, 1:] - position_seq[:, :-1]\n",
    "\n",
    "    # construct the graph based on the distances between particles\n",
    "    n_particle = recent_position.size(0)\n",
    "    edge_index = pyg.nn.radius_graph(recent_position, metadata[\"default_connectivity_radius\"], loop=True, max_num_neighbors=n_particle)\n",
    "    \n",
    "    # node-level features: velocity, distance to the boundary\n",
    "    normal_velocity_seq = (velocity_seq - torch.tensor(metadata[\"vel_mean\"])) / torch.sqrt(torch.tensor(metadata[\"vel_std\"]) ** 2 + noise_std ** 2)\n",
    "    boundary = torch.tensor(metadata[\"bounds\"])\n",
    "    distance_to_lower_boundary = recent_position - boundary[:, 0]\n",
    "    distance_to_upper_boundary = boundary[:, 1] - recent_position\n",
    "    distance_to_boundary = torch.cat((distance_to_lower_boundary, distance_to_upper_boundary), dim=-1)\n",
    "    distance_to_boundary = torch.clip(distance_to_boundary / metadata[\"default_connectivity_radius\"], -1.0, 1.0)\n",
    "\n",
    "    # edge-level features: displacement, distance\n",
    "    dim = recent_position.size(-1)\n",
    "    edge_displacement = (torch.gather(recent_position, dim=0, index=edge_index[0].unsqueeze(-1).expand(-1, dim)) -\n",
    "                   torch.gather(recent_position, dim=0, index=edge_index[1].unsqueeze(-1).expand(-1, dim)))\n",
    "    edge_displacement /= metadata[\"default_connectivity_radius\"]\n",
    "    edge_distance = torch.norm(edge_displacement, dim=-1, keepdim=True)\n",
    "\n",
    "    # ground truth for training\n",
    "    if target_position is not None:\n",
    "        last_velocity = velocity_seq[:, -1]\n",
    "        next_velocity = target_position + position_noise[:, -1] - recent_position\n",
    "        acceleration = next_velocity - last_velocity\n",
    "        acceleration = (acceleration - torch.tensor(metadata[\"acc_mean\"])) / torch.sqrt(torch.tensor(metadata[\"acc_std\"]) ** 2 + noise_std ** 2)\n",
    "    else:\n",
    "        acceleration = None\n",
    "\n",
    "    # return the graph with features\n",
    "    graph = pyg.data.Data(\n",
    "        x=particle_type,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=torch.cat((edge_displacement, edge_distance), dim=-1),\n",
    "        y=acceleration,\n",
    "        pos=torch.cat((velocity_seq.reshape(velocity_seq.size(0), -1), distance_to_boundary), dim=-1)\n",
    "    )\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sort frames\n",
    "def get_sorted_data(data):\n",
    "    '''\n",
    "    data: numpy array: (num_of_frames, number_of_particles, dimension)\n",
    "    return: numpy (num_of_frames, number_of_particles, dimension), with sorted frames\n",
    "    '''\n",
    "    \n",
    "    # class for groups representation: arrays of frames\n",
    "    class ObjectGroup:\n",
    "        def __init__(self, elems):\n",
    "            self.elems = elems\n",
    "        \n",
    "        def add_to_begining(self, elem):\n",
    "            self.elems.insert(0, elem)\n",
    "        \n",
    "        def add_to_end(self, elem):\n",
    "            self.elems.append(elem)\n",
    "\n",
    "        def reverse(self):\n",
    "            self.elems.reverse()\n",
    "\n",
    "    # merge\n",
    "    def merge_groups(g1, g2):\n",
    "        lst = g1.elems\n",
    "        lst.extend(g2.elems)\n",
    "        return ObjectGroup(lst)\n",
    "\n",
    "    # min distance between groups: potentially can be merged front and back \n",
    "    def get_difference(group1, group2):\n",
    "        dist1 = d_matrix[group1.elems[0], group2.elems[0]]\n",
    "        dist2 = d_matrix[group1.elems[0], group2.elems[-1]]\n",
    "        dist3 = d_matrix[group1.elems[-1], group2.elems[0]]\n",
    "        dist4 = d_matrix[group1.elems[-1], group2.elems[-1]]\n",
    "        return min(dist1, dist2, dist3, dist4)\n",
    "\n",
    "    # get index: for flipping\n",
    "    def get_ind(group1, group2):\n",
    "        d = get_difference(group1, group2)\n",
    "        if d == d_matrix[group1.elems[0], group2.elems[0]]:\n",
    "            return 1\n",
    "        if d == d_matrix[group1.elems[0], group2.elems[-1]]:\n",
    "            return 2\n",
    "        if d == d_matrix[group1.elems[-1], group2.elems[0]]:\n",
    "            return 3\n",
    "        if d == d_matrix[group1.elems[-1], group2.elems[-1]]:\n",
    "            return 4\n",
    "        return 4\n",
    "    \n",
    "    # define distance\n",
    "    def get_distance(points_1, points_2):\n",
    "        return np.sum(np.abs(points_1-points_2)**2)**(1./2)\n",
    "\n",
    "    # calculate distances\n",
    "    def calculate_distance_matrix(points):\n",
    "        d_matrix = np.zeros((len(points), len(points)))\n",
    "        for i in range(len(points)):\n",
    "            for j in range(len(points)):\n",
    "                d_matrix[i,j] = get_distance(points[i], points[j])\n",
    "                d_matrix[j,i] = d_matrix[i,j]\n",
    "        #print(np.max(d_matrix))\n",
    "        return d_matrix\n",
    "\n",
    "    # objects:\n",
    "    obj_list = []\n",
    "    p = []\n",
    "    for i in range(data.shape[0]):\n",
    "        oo = ObjectGroup([i])\n",
    "        obj_list.append(oo)\n",
    "        p.append(data[i])\n",
    "\n",
    "    d_matrix = calculate_distance_matrix(p)\n",
    "\n",
    "    # at each iteration we merge sequences until one remains\n",
    "    for i in range(data.shape[0]-1):\n",
    "        ind_1 = -1\n",
    "        ind_2 = -1\n",
    "        min_dist = np.inf\n",
    "        for j in range(len(obj_list)):\n",
    "            for k in range(j+1, len(obj_list)):\n",
    "                dist = get_difference(obj_list[j], obj_list[k])\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    ind_1 = j\n",
    "                    ind_2 = k\n",
    "        which_d = get_ind(obj_list[ind_1], obj_list[ind_2])\n",
    "        \n",
    "        # which lists flip\n",
    "        if which_d == 1 or which_d == 2:\n",
    "            obj_list[ind_1].reverse()\n",
    "        if which_d == 2 or which_d == 4:\n",
    "            obj_list[ind_2].reverse()\n",
    "        obj_1 = obj_list[ind_1]\n",
    "        obj_2 = obj_list[ind_2]\n",
    "        merged = merge_groups(obj_1, obj_2)\n",
    "\n",
    "        # update obj_list\n",
    "        obj_list.remove(obj_1)\n",
    "        obj_list.remove(obj_2)\n",
    "        obj_list.append(merged)\n",
    "    \n",
    "    #print(obj_list[0].elems)\n",
    "    l = obj_list[0].elems\n",
    "    #sorted_d = data[obj_list[0].elems]\n",
    "    if get_distance(data[l[0]], data[l[1]]) > get_distance(data[l[-1]], data[l[-2]]):\n",
    "        l = l[::-1]\n",
    "    \n",
    "    return data[l], l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "MLP is used in a lot of different places throughout the architecture, most notably the encoder and the decoder are both MLPs. We define it as a class to make it easier to use.\n",
    "\n",
    "All MLPs have two hidden layers (with ReLU activations), followed by a nonactivated output layer, each layer with size of 128. All MLPs (except the output decoder) are followed by a LayerNorm layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128, layer_norm=True):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.layer_norm = layer_norm\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        # The rationale behind setting the standard deviation of the normal distribution to 1/sqrt(layer.in_features)\n",
    "        # is to normalize the variance of the layer's inputs and outputs. This helps to prevent the outputs\n",
    "        # from exploding or vanishing during training. The 1/sqrt(layer.in_features) factor is based on the recommendation\n",
    "        # in the paper \"Understanding the difficulty of training deep feedforward neural networks\" by Glorot and Bengio (2010).\n",
    "        self.layer1.weight.data.normal_(0, 1 / torch.sqrt(self.layer1.in_features))\n",
    "        # Setting the bias to 0 allows the network to learn the appropriate bias values during training.\n",
    "        self.layer1.bias.data.fill_(0)\n",
    "        # The same reasoning applies to the other layers.\n",
    "        self.layer2.weight.data.normal_(0, 1 / torch.sqrt(self.layer2.in_features))\n",
    "        self.layer2.bias.data.fill_(0)\n",
    "        self.layer3.weight.data.normal_(0, 1 / torch.sqrt(self.layer3.in_features))\n",
    "        self.layer3.bias.data.fill_(0)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer2(x)\n",
    "        if self.layer_norm:\n",
    "            x = nn.LayerNorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch import load, cat, from_numpy\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "from torch_geometric.data import Dataset\n",
    "\n",
    "class OneStepDataset(Dataset):\n",
    "    def __init__(self, path_metadata, dir_trajectories, split, window_length=7, noise=0.0, return_pos=False) -> None:\n",
    "        super().__init__()\n",
    "        \"\"\"Creates a dataset for one step predictions, using noise. Also sorts the positions to \n",
    "        correct order.\n",
    "        Args:\n",
    "            path_metadata (str): path of directory where metadata for dataset is stored\n",
    "            dir_trajectories (str): path of directory with ONLY directories of trajectories, \n",
    "                which contain corresponding data\n",
    "            split (str, Optional): not yet implemented, currently not in use\n",
    "            window_length (int, Optional): size of window that are looked in parallel when creating a graph\n",
    "            noise (int, Optional): standard deviation that will be used to generate noise to position data\n",
    "            return_pos (bool, Optional): if `True` it returns the last positions in a sequence besides the graph\n",
    "        \"\"\"\n",
    "        with open(join(path_metadata, \"metadata.json\")) as f:\n",
    "            self.metadata = json.load(f)\n",
    "\n",
    "        # helper variable of all dirs containing data for each trajectory\n",
    "        traj_dir = listdir(dir_trajectories)\n",
    "\n",
    "        # particle type tensor for each trajectory, shape = (num_traj, num_particles)\n",
    "        self.particle_type = [\n",
    "            load(join(dir_trajectories, i, 'particle_type.pt')) for i in traj_dir\n",
    "        ]\n",
    "\n",
    "        # particle position tensor for each trajectory, shape = (num_traj, num_frames, num_particles, num_dim)\n",
    "        # ! POSITIONS IN THE TRAIN AND VALID FOLDER ARE ALREADY SORTED\n",
    "        _sorted = True\n",
    "        if not _sorted:\n",
    "            self.positions = [\n",
    "                from_numpy(\n",
    "                    get_sorted_data(\n",
    "                        load(join(dir_trajectories, i, 'positions.pt')).detach().numpy())[0]\n",
    "                )\n",
    "                for i in traj_dir\n",
    "            ]\n",
    "        else:\n",
    "            self.positions = [load(join(dir_trajectories, i, 'positions.pt')) for i in traj_dir]\n",
    "\n",
    "        self.window_length = window_length\n",
    "        self.noise = noise\n",
    "        self.return_pos = return_pos\n",
    "        self.dim = self.positions[0].shape[-1]\n",
    "\n",
    "        # cut trajectories according to time slices\n",
    "        self.windows = []\n",
    "        for i, traj in enumerate(self.positions):\n",
    "            size = traj.shape[1]\n",
    "            length = traj.shape[0] - window_length + 1\n",
    "            for k in range(length):\n",
    "                desc = {\n",
    "                    \"size\": size,\n",
    "                    \"pos\": (i, k),\n",
    "                }\n",
    "                self.windows.append(desc)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        window = self.windows[idx]\n",
    "        size = window[\"size\"]\n",
    "        traj, frame = window[\"pos\"]\n",
    "\n",
    "        particle_type = self.particle_type[traj][:size].detach().clone()\n",
    "        position_seq = self.positions[traj][frame:frame + self.window_length].detach().clone()\n",
    "        # position_seq.resize(self.window_length, size, self.dim)\n",
    "        # position_seq = position_seq.transpose(1, 0, 2)\n",
    "\n",
    "        # target_position = position_seq[:, -1]\n",
    "        # position_seq = position_seq[:, :-1]\n",
    "        \n",
    "        target_position = position_seq[-1, :, :]\n",
    "        position_seq = position_seq[:-1, :, :]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            graph = preprocess(particle_type, position_seq, target_position, self.metadata, self.noise)\n",
    "        if self.return_pos:\n",
    "          return graph, position_seq[:, -1]\n",
    "        return graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN Layer\n",
    "Here we implement InteractionNetwork\\\n",
    "paper: https://proceedings.neurips.cc/paper_files/paper/2016/file/3147da8ab4a0437c15ef51a5cc7f2dc4-Paper.pdf\n",
    "\n",
    "We use MPL that we defined above to generate messages for nodes and edges.\n",
    "\n",
    "Updaed features for nodes, `v_i` and edges, `e_ij`:\n",
    "\n",
    "$$\n",
    "v_i^{k+1} = v_i^k + MLP_n(v_i^k, \\sum_{v_j \\in N(v_i)}{MPL_e(v_i^k, v_j^k, e_ij^k)}) \\\\\n",
    "$$ \n",
    "$$ e_ij^{k+1} = e_ij^k + MLP_e(v_i^k, v_j^k, e_ij^k) $$\n",
    "\n",
    "where $MLP_e(\\cdot, \\cdot, \\cdot)$ is only computed once and then used twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_scatter import scatter\n",
    "from torch import cat\n",
    "\n",
    "class InteractionNetwork(MessagePassing):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.node_msg = MLP(2 * hidden_dim, hidden_dim, hidden_dim)\n",
    "        self.edge_msg = MLP(3 * hidden_dim, hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_feature):\n",
    "        # propagate invokes message() and aggregate(), which return (inputs, out)\n",
    "        # we update edge feature as: current edge feature + current message passing it\n",
    "        edge_out, aggr = self.propagate(edge_index, x=(x, x), edge_feature=edge_feature)\n",
    "        edge_out = edge_feature + edge_out\n",
    "\n",
    "        # we update node features as: sum of neigbouring messages and current\n",
    "        # node feature get passed through coresponding MLP.\n",
    "        # To include self correction a bit we add current feature to that output\n",
    "        node_out = x + self.node_msg(cat((x, aggr), dim=-1))\n",
    "\n",
    "        return node_out, edge_out\n",
    "\n",
    "    def message(self, x_i, x_j, edge_feature):\n",
    "        # here we create messages as an output of MPL with 3 inputs:\n",
    "        # edge feature and feature of each node connected by this edge\n",
    "        x = self.edge_msg(cat((x_i, x_j, edge_feature), dim=-1))\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def aggregate(self, source, index, dim_size=None):\n",
    "        # we sum all neighbouring messages for each node, which we will use to \n",
    "        # update next layer of node features\n",
    "        out = scatter(source, index, dim_size=dim_size, dim=self.node_dim, reduce=\"sum\")\n",
    "\n",
    "        return (source, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Embedding, ModuleList\n",
    "\n",
    "class GNS(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim = 128,\n",
    "        num_types = 9,\n",
    "        emb_dim = 16,\n",
    "        num_gnn_layers = 5,\n",
    "        simulation_dim = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # IMPORTANT: this is the input dimension of data. It means that the model\n",
    "        # gets data from 2 previouos frames(2*sim_dim) plus the embedding.\n",
    "        # this variable is precomputed here for transparency and used in node_input\n",
    "        node_input_dim = 2 * simulation_dim + emb_dim\n",
    "        \n",
    "        # classic torch.nn Embedding\n",
    "        self.embedding = Embedding(num_types, emb_dim)\n",
    "\n",
    "        # node inputs and outputs\n",
    "        self.node_input = MLP(node_input_dim, hidden_dim, hidden_dim)\n",
    "        self.node_outpt = MLP(hidden_dim, hidden_dim, simulation_dim)\n",
    "\n",
    "        self.edge_input = MLP(simulation_dim + 1, hidden_dim, hidden_dim, layer_norm=False)\n",
    "\n",
    "        # initialize gnn layers as InteractionNetwork layers\n",
    "        self.gnns = ModuleList([InteractionNetwork(hidden_dim) for i in range(num_gnn_layers)])\n",
    "\n",
    "        # just save number of layers for later use\n",
    "        self.num_gnns = num_gnn_layers\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # first we embed the data into features\n",
    "        # SELF NOTE: I was guessing that `data` will have keys `x`, `pos`, `edge_attr` and `edge_index`\n",
    "        node_features = self.node_input(cat(self.embedding(data.x), data.pos), dim=-1)\n",
    "        edge_features = self.edge_input(data.edge_attr)\n",
    "\n",
    "        # then propagate them trough model layers\n",
    "        for gnn in self.gnns:\n",
    "            node_features, edge_features = gnn(x=node_features, edge_features=edge_features, edge_index=data.edge_index)\n",
    "\n",
    "        # and finally return node positions, in our case: x, y coordinates\n",
    "        node_output = self.node_outpt(node_features)\n",
    "\n",
    "        return node_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "        'input_dim': 3,\n",
    "        'output_dim': 3,\n",
    "        'hidden_dim': 128,\n",
    "        'layer_norm': True,\n",
    "        'lr': 0.01,\n",
    "        'weight_decay': 5e-3,\n",
    "        'batch_size': 2,\n",
    "        'epochs': 1,\n",
    "        'dropout': 0.5,\n",
    "        'opt': 'adam',\n",
    "        'validate_interval': 1000,\n",
    "        'save_model': True,\n",
    "        'model_path': './model.pt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric as pyg\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_onestep(model: torch.nn.Module, data_loader: pyg.data.Dataset, device: torch.device): # type: ignore\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for batch_number, data in enumerate(data_loader):\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        loss = F.mse_loss(out, data.y)\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / (batch_number + 1) # reportUnboundVariable: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(args: dict[str, Any], model: torch.nn.Module, train_loader, valid_loader):\n",
    "    \n",
    "    # Set the device to GPU if available, otherwise CPU\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    # init optimiser\n",
    "    if args['opt'] == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "    elif args['opt'] == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "    else:\n",
    "        raise ValueError('Unknown optimizer: {}'.format(args['opt']))\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
    "    \n",
    "    # loss function\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # track the losses to be able to plot the learning curve\n",
    "    train_loss = []\n",
    "    validate_loss = []\n",
    "    \n",
    "    # track the total number of steps\n",
    "    steps = 0\n",
    "    \n",
    "    # main train loop\n",
    "    for epoch in range(args['epochs']):\n",
    "        model.train()\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "        \n",
    "        # keep track of the total loss and the number of batches\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for data in progress_bar:\n",
    "            # forward pass\n",
    "            optimizer.zero_grad()\n",
    "            data = data.cuda()\n",
    "            out = model(data)\n",
    "            loss = loss_fn(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # update progress bar\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            progress_bar.set_postfix({\"loss\": loss.item(), \"avg_loss\": total_loss / batch_count})\n",
    "            steps += 1\n",
    "            train_loss.append((steps, loss.item()))\n",
    "\n",
    "            # evaluation\n",
    "            if steps % args[\"validate_interval\"] == 0:\n",
    "                model.eval()\n",
    "                loss = validate_onestep(model, valid_loader, device)\n",
    "                validate_loss.append((steps, validate_loss))\n",
    "                tqdm.write(f\"\\nEval: Loss: {validate_loss}\")\n",
    "                model.train()\n",
    "    \n",
    "    if args['save_model']:\n",
    "        torch.save(model.state_dict(), args['model_path'])\n",
    "    \n",
    "    return train_loss, validate_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the actual initialization of the data, model and training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rok/Documents/uni/mlg/mlg-project/.venv/lib/python3.10/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (678) must match the size of tensor b (6) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m train_loader \u001b[39m=\u001b[39m pyg\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(train_dataset, batch_size\u001b[39m=\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m], drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m valid_loader \u001b[39m=\u001b[39m pyg\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(valid_dataset, batch_size\u001b[39m=\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m], drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m train_loader\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49mget(\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     13\u001b[0m model \u001b[39m=\u001b[39m GNS(\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m) \u001b[39m# TODO: init model to GNS\u001b[39;00m\n\u001b[1;32m     15\u001b[0m train_loss, validate_loss \u001b[39m=\u001b[39m train(args, model, train_loader, valid_loader)\n",
      "Cell \u001b[0;32mIn[5], line 83\u001b[0m, in \u001b[0;36mOneStepDataset.get\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     80\u001b[0m position_seq \u001b[39m=\u001b[39m position_seq[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :, :]\n\u001b[1;32m     82\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 83\u001b[0m     graph \u001b[39m=\u001b[39m preprocess(particle_type, position_seq, target_position, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnoise)\n\u001b[1;32m     84\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_pos:\n\u001b[1;32m     85\u001b[0m   \u001b[39mreturn\u001b[39;00m graph, position_seq[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[2], line 52\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(particle_type, position_seq, target_position, metadata, noise_std)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m target_position \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     last_velocity \u001b[39m=\u001b[39m velocity_seq[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 52\u001b[0m     next_velocity \u001b[39m=\u001b[39m target_position \u001b[39m+\u001b[39;49m position_noise[:, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39m-\u001b[39m recent_position\n\u001b[1;32m     53\u001b[0m     acceleration \u001b[39m=\u001b[39m next_velocity \u001b[39m-\u001b[39m last_velocity\n\u001b[1;32m     54\u001b[0m     acceleration \u001b[39m=\u001b[39m (acceleration \u001b[39m-\u001b[39m torch\u001b[39m.\u001b[39mtensor(metadata[\u001b[39m\"\u001b[39m\u001b[39macc_mean\u001b[39m\u001b[39m\"\u001b[39m])) \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39msqrt(torch\u001b[39m.\u001b[39mtensor(metadata[\u001b[39m\"\u001b[39m\u001b[39macc_std\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m+\u001b[39m noise_std \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (678) must match the size of tensor b (6) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "# here the actual training takes place\n",
    "train_dataset = OneStepDataset(\"dataset/WaterDropSample/\", \"data/train\", \"\") # TODO: init train dataset\n",
    "valid_dataset = OneStepDataset(\"dataset/WaterDropSample/\", \"data/valid\", \"\") # TODO: init valid dataset\n",
    "\n",
    "# train_loader = pyg.data.DataLoader(train_dataset, batch_size=args['batch_size'], drop_last=True, shuffle=True, pin_memory=True, num_workers=8)\n",
    "# valid_loader = pyg.data.DataLoader(valid_dataset, batch_size=args['batch_size'], drop_last=True, shuffle=False, pin_memory=True, num_workers=8)\n",
    "\n",
    "train_loader = pyg.data.DataLoader(train_dataset, batch_size=args['batch_size'], drop_last=True, shuffle=False)\n",
    "valid_loader = pyg.data.DataLoader(valid_dataset, batch_size=args['batch_size'], drop_last=True, shuffle=False)\n",
    "\n",
    "train_loader.dataset.get(0)\n",
    "\n",
    "model = GNS(2,2,2,2) # TODO: init model to GNS\n",
    "\n",
    "train_loss, validate_loss = train(args, model, train_loader, valid_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39m# visualize the loss curve\u001b[39;00m\n\u001b[0;32m      5\u001b[0m plt\u001b[39m.\u001b[39mfigure()\n\u001b[1;32m----> 6\u001b[0m plt\u001b[39m.\u001b[39mplot(\u001b[39m*\u001b[39m\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mtrain_loss), label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[39m.\u001b[39mplot(\u001b[39m*\u001b[39m\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mvalidate_loss), label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loss' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualize the loss curve\n",
    "plt.figure()\n",
    "plt.plot(*zip(*train_loss), label=\"train\")\n",
    "plt.plot(*zip(*validate_loss), label=\"valid\")\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort the frames in the data to be in correct order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexes import *\n",
    "import os\n",
    "\n",
    "os.makedirs('./data/train', exist_ok=True)\n",
    "os.makedirs('./data/valid', exist_ok=True)\n",
    "\n",
    "train_indexes = [0,1,2]\n",
    "valid_indexes = [3,4]\n",
    "\n",
    "indexes = [index_0, index_1, index_2, index_3, index_4]\n",
    "\n",
    "for train_index in train_indexes:\n",
    "    positions = torch.load('./data/trajectories/{}/positions.pt'.format(train_index))\n",
    "    labels = torch.load('./data/trajectories/{}/labels.pt'.format(train_index))\n",
    "    particle_type = torch.load('./data/trajectories/{}/particle_type.pt'.format(train_index))\n",
    "    positions = positions[indexes[train_index]]\n",
    "    labels = labels[indexes[train_index]]\n",
    "    particle_type = particle_type[indexes[train_index]]\n",
    "    os.makedirs('./data/train/{}'.format(train_index), exist_ok=True)\n",
    "    with open('./data/train/{}/positions.pt'.format(train_index), 'wb') as f:\n",
    "        torch.save(positions, f)\n",
    "    with open('./data/train/{}/labels.pt'.format(train_index), 'wb') as f:\n",
    "        torch.save(labels, f)\n",
    "    with open('./data/train/{}/particle_type.pt'.format(train_index), 'wb') as f:\n",
    "        torch.save(particle_type, f)\n",
    "\n",
    "for valid_index in valid_indexes:\n",
    "    positions = torch.load('./data/trajectories/{}/positions.pt'.format(valid_index))\n",
    "    labels = torch.load('./data/trajectories/{}/labels.pt'.format(valid_index))\n",
    "    particle_type = torch.load('./data/trajectories/{}/particle_type.pt'.format(valid_index))\n",
    "    positions = positions[indexes[valid_index]]\n",
    "    labels = labels[indexes[valid_index]]\n",
    "    particle_type = particle_type[indexes[valid_index]]\n",
    "    os.makedirs('./data/valid/{}'.format(valid_index), exist_ok=True)\n",
    "    with open('./data/valid/{}/positions.pt'.format(valid_index), 'wb') as f:\n",
    "        torch.save(positions, f)\n",
    "    with open('./data/valid/{}/labels.pt'.format(valid_index), 'wb') as f:\n",
    "        torch.save(labels, f)\n",
    "    with open('./data/valid/{}/particle_type.pt'.format(valid_index), 'wb') as f:\n",
    "        torch.save(particle_type, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten the position seqence to only have one dimension (current position) for each time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixed 0\n",
      "fixed 1\n",
      "fixed 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "data_type = ('train', 'valid')\n",
    "trajectory_path = os.path.join('data', data_type[0])\n",
    "for trajectory_i in os.listdir(trajectory_path):\n",
    "    position_seq = torch.load(os.path.join(trajectory_path, trajectory_i, 'positions.pt'))\n",
    "    if len(position_seq.shape) == 3:\n",
    "        continue\n",
    "    # from the first frame (dimension 0)\n",
    "    # for all of the particles (dimension 1)\n",
    "    # get the first 5 positions (dimension 2)\n",
    "    # and the x and y coordinates (dimension 3)\n",
    "    # and extract them in a tensor of shape (5, num_particles, 2)\n",
    "    first_5_positions = position_seq[0, :, :5, :]\n",
    "    first_5_positions = first_5_positions.permute(1, 0, 2)\n",
    "    # for all the other frames\n",
    "    # extract only the last position\n",
    "    # and join them to the first 5 positions\n",
    "    # to get the final shape of (num_frames = 1000, num_particles, 2)\n",
    "    last_positions = position_seq[:, :, -1, :]\n",
    "    final_positions = torch.cat((first_5_positions, last_positions), dim=0)\n",
    "    # save the final positions\n",
    "    with open(os.path.join(trajectory_path, trajectory_i, 'positions.pt'), 'wb') as f:\n",
    "        print(f'fixed {trajectory_i}')\n",
    "        torch.save(final_positions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
