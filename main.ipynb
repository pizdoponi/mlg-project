{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset\n",
        "\n",
        "We used a subset of WaterDrop dataset from Deepmind. The videos only covers the specific case of a water droplet in vacuum, but that is fine with us, as that is exactly what we wanted to model!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Converting TFRecord to torch tensors\n",
        "\n",
        "Unfortunately, the dataset is not available in a format that is easy to use with PyTorch. We need to convert it to a format that is more suitable for PyTorch (e.g. `torch.Tensor`).\n",
        "\n",
        "We achieve this by iterating over the TFRecord file, extracting the features for each frame and adding this information to its corresponding trajctory. We then save the trajectories as a pickle file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1186,
      "metadata": {},
      "outputs": [],
      "source": [
        "from read_dataset import prepare_data_from_tfds\n",
        "import torch\n",
        "\n",
        "generate_trajectories = False\n",
        "\n",
        "if generate_trajectories:\n",
        "    # initialize dataset\n",
        "    # this is of type tf.data.Dataset\n",
        "    ds = prepare_data_from_tfds(data_path='dataset/WaterDrop/train.tfrecord', is_rollout=False, batch_size=1)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # kepp track of the number of batches we iterate over in the dataset\n",
        "    batch_count = 0\n",
        "    # there are many trajectories in the dataset, we want to save each trajectory in a separate folder\n",
        "    number_of_trajectories = 0\n",
        "    number_of_frames_in_trajectory = {}\n",
        "\n",
        "    # this are the features we want to extract from the dataset\n",
        "    positions = {}\n",
        "    particle_type = {}\n",
        "    labels = {}\n",
        "\n",
        "    print('started loading data...')\n",
        "    for features, current_labels in ds:\n",
        "        \n",
        "        # extract feature\n",
        "        number_of_particles = features['n_particles_per_example'][0]\n",
        "        current_positions = torch.tensor(features['position']).to(device)\n",
        "        current_labels = torch.tensor(current_labels).to(device)\n",
        "        \n",
        "        # if this is the first time we see this number of particles\n",
        "        # i.e. this is a new trajectory\n",
        "        if number_of_particles not in positions.keys():\n",
        "            number_of_trajectories += 1\n",
        "            number_of_frames_in_trajectory[number_of_particles] = 1\n",
        "            positions[number_of_particles] = current_positions\n",
        "            labels[number_of_particles] = current_labels\n",
        "            particle_type[number_of_particles] = torch.tensor(features['particle_type']).to(device)\n",
        "        \n",
        "        else:\n",
        "            # and if else is required here beacuse we need to stack the first two frames, and only than we can cat\n",
        "            if number_of_frames_in_trajectory[number_of_particles] == 1:\n",
        "                positions[number_of_particles] = torch.stack((positions[number_of_particles], current_positions), dim=0)\n",
        "                labels[number_of_particles] = torch.stack((labels[number_of_particles], current_labels), dim=0)\n",
        "                particle_type[number_of_particles] = torch.stack((particle_type[number_of_particles], torch.tensor(features['particle_type']).to(device)), dim=0)\n",
        "            \n",
        "            else:\n",
        "                positions[number_of_particles] = torch.cat((positions[number_of_particles], current_positions.unsqueeze(0)), dim=0)\n",
        "                labels[number_of_particles] = torch.cat((labels[number_of_particles], current_labels.unsqueeze(0)), dim=0)\n",
        "                particle_type[number_of_particles] = torch.cat((particle_type[number_of_particles], torch.tensor(features['particle_type']).to(device).unsqueeze(0)), dim=0)\n",
        "                \n",
        "            number_of_frames_in_trajectory[number_of_particles] += 1\n",
        "        \n",
        "        print(f'batch {batch_count} done; number of trajectories: {number_of_trajectories}')\n",
        "        batch_count += 1\n",
        "        \n",
        "        # the main way to end the loop\n",
        "        # change this if you want a different number of trajectories\n",
        "        # note: trajectories are randomly mixes and do not appear in the\n",
        "        if number_of_trajectories == 22:\n",
        "            break\n",
        "        \n",
        "    print('all batches done')\n",
        "    print('converting to tensors...')\n",
        "\n",
        "    import shutil\n",
        "    import os\n",
        "\n",
        "    trajectory_dir_count = 0\n",
        "    for trajectory_dir_count, n_particles in enumerate(positions.keys()):\n",
        "        if os.path.exists(f'data/trajectories/{trajectory_dir_count}'):\n",
        "            shutil.rmtree(f'data/trajectories/{trajectory_dir_count}')\n",
        "            print(f'data/trajectories/{trajectory_dir_count} deleted')\n",
        "            \n",
        "        os.makedirs(f'data/trajectories/{trajectory_dir_count}')\n",
        "        print(f'data/trajectories/{trajectory_dir_count} created')\n",
        "        \n",
        "        with open(f'data/trajectories/{trajectory_dir_count}/positions.pt', 'wb') as f:\n",
        "            torch.save(positions[n_particles], f)\n",
        "            print(f'data/trajectories/{trajectory_dir_count}/positions.pt saved')\n",
        "            \n",
        "        with open(f'data/trajectories/{trajectory_dir_count}/labels.pt', 'wb') as f:\n",
        "            torch.save(labels[n_particles], f)\n",
        "            print(f'data/trajectories/{trajectory_dir_count}/labels.pt saved')\n",
        "            \n",
        "        with open(f'data/trajectories/{trajectory_dir_count}/particle_type.pt', 'wb') as f:\n",
        "            torch.save(particle_type[n_particles], f)\n",
        "            print(f'data/trajectories/{trajectory_dir_count}/particle_type.pt saved')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sorting the frames\n",
        "Below is the function that calculates the permutation that sorts the data by the frame number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# sort frames\n",
        "def get_sorted_data(data):\n",
        "    '''\n",
        "    data: numpy array: (num_of_frames, number_of_particles, dimension)\n",
        "    return: numpy (num_of_frames, number_of_particles, dimension), with sorted frames\n",
        "    '''\n",
        "    \n",
        "    # class for groups representation: arrays of frames\n",
        "    class ObjectGroup:\n",
        "        def __init__(self, elems):\n",
        "            self.elems = elems\n",
        "        \n",
        "        def add_to_begining(self, elem):\n",
        "            self.elems.insert(0, elem)\n",
        "        \n",
        "        def add_to_end(self, elem):\n",
        "            self.elems.append(elem)\n",
        "\n",
        "        def reverse(self):\n",
        "            self.elems.reverse()\n",
        "\n",
        "    # merge\n",
        "    def merge_groups(g1, g2):\n",
        "        lst = g1.elems\n",
        "        lst.extend(g2.elems)\n",
        "        return ObjectGroup(lst)\n",
        "\n",
        "    # min distance between groups: potentially can be merged front and back \n",
        "    def get_difference(group1, group2):\n",
        "        dist1 = d_matrix[group1.elems[0], group2.elems[0]]\n",
        "        dist2 = d_matrix[group1.elems[0], group2.elems[-1]]\n",
        "        dist3 = d_matrix[group1.elems[-1], group2.elems[0]]\n",
        "        dist4 = d_matrix[group1.elems[-1], group2.elems[-1]]\n",
        "        return min(dist1, dist2, dist3, dist4)\n",
        "\n",
        "    # get index: for flipping\n",
        "    def get_ind(group1, group2):\n",
        "        d = get_difference(group1, group2)\n",
        "        if d == d_matrix[group1.elems[0], group2.elems[0]]:\n",
        "            return 1\n",
        "        if d == d_matrix[group1.elems[0], group2.elems[-1]]:\n",
        "            return 2\n",
        "        if d == d_matrix[group1.elems[-1], group2.elems[0]]:\n",
        "            return 3\n",
        "        if d == d_matrix[group1.elems[-1], group2.elems[-1]]:\n",
        "            return 4\n",
        "        return 4\n",
        "    \n",
        "    # define distance\n",
        "    def get_distance(points_1, points_2):\n",
        "        return np.sum(np.abs(points_1-points_2)**2)**(1./2)\n",
        "\n",
        "    # calculate distances\n",
        "    def calculate_distance_matrix(points):\n",
        "        d_matrix = np.zeros((len(points), len(points)))\n",
        "        for i in range(len(points)):\n",
        "            for j in range(len(points)):\n",
        "                d_matrix[i,j] = get_distance(points[i], points[j])\n",
        "                d_matrix[j,i] = d_matrix[i,j]\n",
        "        #print(np.max(d_matrix))\n",
        "        return d_matrix\n",
        "\n",
        "    # objects:\n",
        "    obj_list = []\n",
        "    p = []\n",
        "    for i in range(data.shape[0]):\n",
        "        oo = ObjectGroup([i])\n",
        "        obj_list.append(oo)\n",
        "        p.append(data[i])\n",
        "\n",
        "    d_matrix = calculate_distance_matrix(p)\n",
        "\n",
        "    # at each iteration we merge sequences until one remains\n",
        "    for i in range(data.shape[0]-1):\n",
        "        ind_1 = -1\n",
        "        ind_2 = -1\n",
        "        min_dist = np.inf\n",
        "        for j in range(len(obj_list)):\n",
        "            for k in range(j+1, len(obj_list)):\n",
        "                dist = get_difference(obj_list[j], obj_list[k])\n",
        "                if dist < min_dist:\n",
        "                    min_dist = dist\n",
        "                    ind_1 = j\n",
        "                    ind_2 = k\n",
        "        which_d = get_ind(obj_list[ind_1], obj_list[ind_2])\n",
        "        \n",
        "        # which lists flip\n",
        "        if which_d == 1 or which_d == 2:\n",
        "            obj_list[ind_1].reverse()\n",
        "        if which_d == 2 or which_d == 4:\n",
        "            obj_list[ind_2].reverse()\n",
        "        obj_1 = obj_list[ind_1]\n",
        "        obj_2 = obj_list[ind_2]\n",
        "        merged = merge_groups(obj_1, obj_2)\n",
        "\n",
        "        # update obj_list\n",
        "        obj_list.remove(obj_1)\n",
        "        obj_list.remove(obj_2)\n",
        "        obj_list.append(merged)\n",
        "    \n",
        "    #print(obj_list[0].elems)\n",
        "    l = obj_list[0].elems\n",
        "    #sorted_d = data[obj_list[0].elems]\n",
        "    if get_distance(data[l[0]], data[l[1]]) > get_distance(data[l[-1]], data[l[-2]]):\n",
        "        l = l[::-1]\n",
        "    \n",
        "    return data[l], l"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have the permutations, we can sort the data. For convenience, we have included the permutation indexes for the first 20 trajectories. This way, you don't have to wait for the sorting to finish. You can find them in `permutations.py`. Each permutation is a list of integers, where the index of the integer is the frame number and the value is the index of the frame in the original trajectory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18\n"
          ]
        }
      ],
      "source": [
        "import permutations\n",
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "\n",
        "shutil.rmtree('./data/train')\n",
        "shutil.rmtree('./data/valid')\n",
        "shutil.rmtree('./data/test')\n",
        "\n",
        "os.makedirs('./data/train', exist_ok=True)\n",
        "os.makedirs('./data/valid', exist_ok=True)\n",
        "os.makedirs('./data/test', exist_ok=True)\n",
        "\n",
        "# for each of the train, valid, test specify what trajectories to use\n",
        "n_train = 12\n",
        "n_valid = 3\n",
        "n_test = 3\n",
        "train_indexes = [i for i in range(n_train)]\n",
        "valid_indexes = [i for i in range(n_train, n_train+n_valid)]\n",
        "test_indexes = [i for i in range(n_train+n_valid, n_train+n_valid+n_test)]\n",
        "\n",
        "indexes = permutations.indexes\n",
        "\n",
        "print(len(indexes))\n",
        "for i in range(len(indexes)):\n",
        "    for j in range(995,1000):\n",
        "        try:\n",
        "            indexes[i].remove(j)\n",
        "        except:\n",
        "            print(i, j)\n",
        "\n",
        "for train_index in train_indexes:\n",
        "    positions = torch.load('./data/trajectories/{}/positions.pt'.format(train_index))\n",
        "    labels = torch.load('./data/trajectories/{}/labels.pt'.format(train_index))\n",
        "    particle_type = torch.load('./data/trajectories/{}/particle_type.pt'.format(train_index))\n",
        "    positions = positions[indexes[train_index]]\n",
        "    labels = labels[indexes[train_index]]\n",
        "    particle_type = particle_type[indexes[train_index]]\n",
        "    os.makedirs('./data/train/{}'.format(train_index), exist_ok=True)\n",
        "    with open('./data/train/{}/positions.pt'.format(train_index), 'wb') as f:\n",
        "        torch.save(positions, f)\n",
        "    with open('./data/train/{}/labels.pt'.format(train_index), 'wb') as f:\n",
        "        torch.save(labels, f)\n",
        "    with open('./data/train/{}/particle_type.pt'.format(train_index), 'wb') as f:\n",
        "        torch.save(particle_type, f)\n",
        "\n",
        "for valid_index in valid_indexes:\n",
        "    positions = torch.load('./data/trajectories/{}/positions.pt'.format(valid_index))\n",
        "    labels = torch.load('./data/trajectories/{}/labels.pt'.format(valid_index))\n",
        "    particle_type = torch.load('./data/trajectories/{}/particle_type.pt'.format(valid_index))\n",
        "    positions = positions[indexes[valid_index]]\n",
        "    labels = labels[indexes[valid_index]]\n",
        "    particle_type = particle_type[indexes[valid_index]]\n",
        "    os.makedirs('./data/valid/{}'.format(valid_index), exist_ok=True)\n",
        "    with open('./data/valid/{}/positions.pt'.format(valid_index), 'wb') as f:\n",
        "        torch.save(positions, f)\n",
        "    with open('./data/valid/{}/labels.pt'.format(valid_index), 'wb') as f:\n",
        "        torch.save(labels, f)\n",
        "    with open('./data/valid/{}/particle_type.pt'.format(valid_index), 'wb') as f:\n",
        "        torch.save(particle_type, f)\n",
        "\n",
        "for test_index in test_indexes:\n",
        "    positions = torch.load('./data/trajectories/{}/positions.pt'.format(test_index))\n",
        "    labels = torch.load('./data/trajectories/{}/labels.pt'.format(test_index))\n",
        "    particle_type = torch.load('./data/trajectories/{}/particle_type.pt'.format(test_index))\n",
        "    positions = positions[indexes[test_index]]\n",
        "    labels = labels[indexes[test_index]]\n",
        "    particle_type = particle_type[indexes[test_index]]\n",
        "    os.makedirs('./data/test/{}'.format(test_index), exist_ok=True)\n",
        "    with open('./data/test/{}/positions.pt'.format(test_index), 'wb') as f:\n",
        "        torch.save(positions, f)\n",
        "    with open('./data/test/{}/labels.pt'.format(test_index), 'wb') as f:\n",
        "        torch.save(labels, f)\n",
        "    with open('./data/test/{}/particle_type.pt'.format(test_index), 'wb') as f:\n",
        "        torch.save(particle_type, f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting the positions\n",
        "\n",
        "The last step of our data preparation step is to extract the positions. In the dataset, for each frame and for every particle, there is a window of size 6, which contains the information about the previous positions. We want to extract it in such a way, that each position tensor will have a shape of size `(<number of frames = 1000>, <number of particles>, <dimension> = 2)`. This way we have the complete freedom to choose the window size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fixed 0\n",
            "fixed 1\n",
            "fixed 10\n",
            "fixed 11\n",
            "fixed 2\n",
            "fixed 3\n",
            "fixed 4\n",
            "fixed 5\n",
            "fixed 6\n",
            "fixed 7\n",
            "fixed 8\n",
            "fixed 9\n",
            "fixed 12\n",
            "fixed 13\n",
            "fixed 14\n",
            "fixed 15\n",
            "fixed 16\n",
            "fixed 17\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "data_types = ('train', 'valid', 'test')\n",
        "for data_type in data_types:\n",
        "    trajectory_path = os.path.join('data', data_type)\n",
        "    for trajectory_i in os.listdir(trajectory_path):\n",
        "        position_seq = torch.load(os.path.join(trajectory_path, trajectory_i, 'positions.pt'))\n",
        "        if len(position_seq.shape) == 3:\n",
        "            continue\n",
        "        # from the first frame (dimension 0)\n",
        "        # for all of the particles (dimension 1)\n",
        "        # get the first 5 positions (dimension 2)\n",
        "        # and the x and y coordinates (dimension 3)\n",
        "        # and extract them in a tensor of shape (5, num_particles, 2)\n",
        "        first_5_positions = position_seq[0, :, :5, :]\n",
        "        first_5_positions = first_5_positions.permute(1, 0, 2)\n",
        "        # for all the other frames\n",
        "        # extract only the last position\n",
        "        # and join them to the first 5 positions\n",
        "        # to get the final shape of (num_frames = 1000, num_particles, 2)\n",
        "        last_positions = position_seq[:, :, -1, :]\n",
        "        final_positions = torch.cat((first_5_positions, last_positions), dim=0)\n",
        "        # save the final positions\n",
        "        with open(os.path.join(trajectory_path, trajectory_i, 'positions.pt'), 'wb') as f:\n",
        "            print(f'fixed {trajectory_i}')\n",
        "            torch.save(final_positions, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch_geometric as pyg\n",
        "\n",
        "def generate_noise(position_seq, noise_std=3e-4):\n",
        "    \"\"\"Generate noise for a trajectory\"\"\"\n",
        "    velocity_seq = position_seq[:, 1:] - position_seq[:, :-1]\n",
        "    time_steps = velocity_seq.size(1)\n",
        "    velocity_noise = torch.randn_like(velocity_seq) * (noise_std / time_steps ** 0.5)\n",
        "\n",
        "    # restrict possible values: |velocity_noise[i,j,k]| <= (noise_std / time_steps ** 0.5)\n",
        "    velocity_noise = torch.minimum(velocity_noise, torch.ones(velocity_noise.shape) * (noise_std / time_steps ** 0.5))\n",
        "    velocity_noise = torch.maximum(velocity_noise, -torch.ones(velocity_noise.shape) * (noise_std / time_steps ** 0.5))\n",
        "    \n",
        "    velocity_noise = velocity_noise.cumsum(dim=1)\n",
        "    position_noise = velocity_noise.cumsum(dim=1)\n",
        "    position_noise = torch.cat((torch.zeros_like(position_noise)[:, 0:1], position_noise), dim=1)\n",
        "    return position_noise\n",
        "\n",
        "\n",
        "def preprocess(particle_type, position_seq, target_position, metadata, noise_std=3e-4):\n",
        "    \"\"\"Preprocess a trajectory and construct the graph\"\"\"\n",
        "    # apply noise to the trajectory\n",
        "    position_noise = generate_noise(position_seq, noise_std)\n",
        "    position_seq = position_seq + position_noise\n",
        "\n",
        "    # calculate the velocities of particles\n",
        "    recent_position = position_seq[:, -1]\n",
        "    velocity_seq = position_seq[:, 1:] - position_seq[:, :-1]\n",
        "\n",
        "    # construct the graph based on the distances between particles\n",
        "    n_particle = recent_position.size(0)\n",
        "    edge_index = pyg.nn.radius_graph(recent_position, metadata[\"default_connectivity_radius\"], loop=True, max_num_neighbors=n_particle)\n",
        "    \n",
        "    # node-level features: velocity, distance to the boundary\n",
        "    normal_velocity_seq = (velocity_seq - torch.tensor(metadata[\"vel_mean\"])) / torch.sqrt(torch.tensor(metadata[\"vel_std\"]) ** 2 + noise_std ** 2)\n",
        "    boundary = torch.tensor(metadata[\"bounds\"])\n",
        "    distance_to_lower_boundary = recent_position - boundary[:, 0]\n",
        "    distance_to_upper_boundary = boundary[:, 1] - recent_position\n",
        "    distance_to_boundary = torch.cat((distance_to_lower_boundary, distance_to_upper_boundary), dim=-1)\n",
        "    distance_to_boundary = torch.clip(distance_to_boundary / metadata[\"default_connectivity_radius\"], -1.0, 1.0)\n",
        "\n",
        "    # edge-level features: displacement, distance\n",
        "    dim = recent_position.size(-1)\n",
        "    edge_displacement = (torch.gather(recent_position, dim=0, index=edge_index[0].unsqueeze(-1).expand(-1, dim)) -\n",
        "                   torch.gather(recent_position, dim=0, index=edge_index[1].unsqueeze(-1).expand(-1, dim)))\n",
        "    edge_displacement /= metadata[\"default_connectivity_radius\"]\n",
        "    edge_distance = torch.norm(edge_displacement, dim=-1, keepdim=True)\n",
        "\n",
        "    # ground truth for training\n",
        "    if target_position is not None:\n",
        "        last_velocity = velocity_seq[:, -1]\n",
        "        next_velocity = target_position + position_noise[:, -1] - recent_position\n",
        "        acceleration = next_velocity - last_velocity\n",
        "        acceleration = (acceleration - torch.tensor(metadata[\"acc_mean\"])) / torch.sqrt(torch.tensor(metadata[\"acc_std\"]) ** 2 + noise_std ** 2)\n",
        "    else:\n",
        "        acceleration = None\n",
        "\n",
        "    # return the graph with features\n",
        "    graph = pyg.data.Data(\n",
        "        x=particle_type,\n",
        "        edge_index=edge_index,\n",
        "        edge_attr=torch.cat((edge_displacement, edge_distance), dim=-1),\n",
        "        y=acceleration,\n",
        "        pos=torch.cat((velocity_seq.reshape(velocity_seq.size(0), -1), distance_to_boundary), dim=-1)\n",
        "    )\n",
        "    return graph"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset object\n",
        "\n",
        "We create a custom dataset object that will be used to load the data. The dataset object will be used by the dataloader to load the data in batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from torch import load, cat, from_numpy\n",
        "from os.path import join\n",
        "from os import listdir\n",
        "from torch_geometric.data import Dataset\n",
        "\n",
        "class OneStepDataset(Dataset):\n",
        "    def __init__(self, path_metadata, dir_trajectories, split=\"\", window_length=7, noise=0.0, return_pos=False) -> None:\n",
        "        super().__init__()\n",
        "        \"\"\"Creates a dataset for one step predictions, using noise. Also sorts the positions to \n",
        "        correct order.\n",
        "        Args:\n",
        "            path_metadata (str): path of directory where metadata for dataset is stored\n",
        "            dir_trajectories (str): path of directory with ONLY directories of trajectories, \n",
        "                which contain corresponding data\n",
        "            split (str, Optional): not yet implemented, currently not in use\n",
        "            window_length (int, Optional): size of window that are looked in parallel when creating a graph\n",
        "            noise (int, Optional): standard deviation that will be used to generate noise to position data\n",
        "            return_pos (bool, Optional): if `True` it returns the last positions in a sequence besides the graph\n",
        "        \"\"\"\n",
        "        with open(join(path_metadata, \"metadata.json\")) as f:\n",
        "            self.metadata = json.load(f)\n",
        "\n",
        "        # helper variable of all dirs containing data for each trajectory\n",
        "        traj_dir = listdir(dir_trajectories)\n",
        "\n",
        "        # particle type tensor for each trajectory, shape = (num_traj, num_particles)\n",
        "        self.particle_type = [\n",
        "            load(join(dir_trajectories, i, 'particle_type.pt')) for i in traj_dir\n",
        "        ]\n",
        "\n",
        "        # particle position tensor for each trajectory, shape = (num_traj, num_frames, num_particles, num_dim)\n",
        "        # ! POSITIONS IN THE TRAIN AND VALID FOLDER ARE ALREADY SORTED\n",
        "        _sorted = True\n",
        "        if not _sorted:\n",
        "            self.positions = [\n",
        "                from_numpy(\n",
        "                    get_sorted_data(\n",
        "                        load(join(dir_trajectories, i, 'positions.pt')).detach().numpy())[0]\n",
        "                )\n",
        "                for i in traj_dir\n",
        "            ]\n",
        "        else:\n",
        "            self.positions = [load(join(dir_trajectories, i, 'positions.pt')) for i in traj_dir]\n",
        "\n",
        "        self.window_length = window_length\n",
        "        self.noise = noise\n",
        "        self.return_pos = return_pos\n",
        "        self.dim = self.positions[0].shape[-1]\n",
        "\n",
        "        # cut trajectories according to time slices\n",
        "        self.windows = []\n",
        "        for i, traj in enumerate(self.positions):\n",
        "            size = traj.shape[1]\n",
        "            length = traj.shape[0] - window_length + 1\n",
        "            for k in range(length):\n",
        "                desc = {\n",
        "                    \"size\": size,\n",
        "                    \"pos\": (i, k),\n",
        "                }\n",
        "                self.windows.append(desc)\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.windows)\n",
        "    \n",
        "    def get(self, idx):\n",
        "        window = self.windows[idx]\n",
        "        size = window[\"size\"]\n",
        "        traj, frame = window[\"pos\"]\n",
        "\n",
        "        particle_type = self.particle_type[traj][0].detach().clone()\n",
        "        position_seq = self.positions[traj][frame:frame + self.window_length].detach().clone()\n",
        "        position_seq = position_seq.permute(1, 0, 2)\n",
        "        target_position = position_seq[:, -1]\n",
        "        position_seq = position_seq[:, :-1]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            graph = preprocess(particle_type, position_seq, target_position, self.metadata, self.noise)\n",
        "        if self.return_pos:\n",
        "          return graph, position_seq[:, -1]\n",
        "        return graph"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GNN model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MLP\n",
        "\n",
        "MLP is used in a lot of different places throughout the architecture, most notably the encoder and the decoder are both MLPs. We define it as a class to make it easier to use.\n",
        "\n",
        "All MLPs have two hidden layers (with ReLU activations), followed by a nonactivated output layer, each layer with size of 128. All MLPs (except the output decoder) are followed by a LayerNorm layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, output_dim, hidden_dim=128, layer_norm=True):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.layer3 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.layerNorm = nn.LayerNorm(output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer_norm = layer_norm\n",
        "    \n",
        "    def reset_parameters(self):\n",
        "        # The rationale behind setting the standard deviation of the normal distribution to 1/sqrt(layer.in_features)\n",
        "        # is to normalize the variance of the layer's inputs and outputs. This helps to prevent the outputs\n",
        "        # from exploding or vanishing during training. The 1/sqrt(layer.in_features) factor is based on the recommendation\n",
        "        # in the paper \"Understanding the difficulty of training deep feedforward neural networks\" by Glorot and Bengio (2010).\n",
        "        self.layer1.weight.data.normal_(0, 1 / torch.sqrt(self.layer1.in_features))\n",
        "        # Setting the bias to 0 allows the network to learn the appropriate bias values during training.\n",
        "        self.layer1.bias.data.fill_(0)\n",
        "        # The same reasoning applies to the other layers.\n",
        "        self.layer2.weight.data.normal_(0, 1 / torch.sqrt(self.layer2.in_features))\n",
        "        self.layer2.bias.data.fill_(0)\n",
        "        self.layer3.weight.data.normal_(0, 1 / torch.sqrt(self.layer3.in_features))\n",
        "        self.layer3.bias.data.fill_(0)\n",
        "        \n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.relu(x)\n",
        "        if self.layer_norm:\n",
        "            x = self.layerNorm(x)\n",
        "        return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GNN Layer\n",
        "Here we implement InteractionNetwork\\\n",
        "paper: https://proceedings.neurips.cc/paper_files/paper/2016/file/3147da8ab4a0437c15ef51a5cc7f2dc4-Paper.pdf\n",
        "\n",
        "We use MPL that we defined above to generate messages for nodes and edges.\n",
        "\n",
        "Updaed features for nodes, `v_i` and edges, `e_ij`:\n",
        "\n",
        "$$\n",
        "v_i^{k+1} = v_i^k + MLP_n(v_i^k, \\sum_{v_j \\in N(v_i)}{MPL_e(v_i^k, v_j^k, e_ij^k)}) \\\\\n",
        "$$ \n",
        "$$ e_{ij}^{k+1} = e_{ij}^k + MLP_e(v_i^k, v_j^k, e_{ij}^k) $$\n",
        "\n",
        "where $MLP_e(\\cdot, \\cdot, \\cdot)$ is only computed once and then used twice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_scatter import scatter\n",
        "from torch import cat\n",
        "\n",
        "class InteractionNetwork(MessagePassing):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.node_msg = MLP(2 * hidden_dim, hidden_dim, hidden_dim)\n",
        "        self.edge_msg = MLP(3 * hidden_dim, hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_feature):\n",
        "        # propagate invokes message() and aggregate(), which return (inputs, out)\n",
        "        # we update edge feature as: current edge feature + current message passing it\n",
        "        edge_out, aggr = self.propagate(edge_index, x=(x, x), edge_feature=edge_feature)\n",
        "        edge_out = edge_feature + edge_out\n",
        "\n",
        "        # we update node features as: sum of neigbouring messages and current\n",
        "        # node feature get passed through coresponding MLP.\n",
        "        # To include self correction a bit we add current feature to that output\n",
        "        node_out = x + self.node_msg(cat((x, aggr), dim=-1))\n",
        "\n",
        "        return node_out, edge_out\n",
        "\n",
        "    def message(self, x_i, x_j, edge_feature):\n",
        "        # here we create messages as an output of MPL with 3 inputs:\n",
        "        # edge feature and feature of each node connected by this edge\n",
        "        x = self.edge_msg(cat((x_i, x_j, edge_feature), dim=-1))\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def aggregate(self, source, index, dim_size=None):\n",
        "        # we sum all neighbouring messages for each node, which we will use to \n",
        "        # update next layer of node features\n",
        "        out = scatter(source, index, dim_size=dim_size, dim=self.node_dim, reduce=\"sum\")\n",
        "\n",
        "        return (source, out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.nn import Embedding, ModuleList\n",
        "from torch import nn\n",
        "\n",
        "class GNS(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_dim = 128,\n",
        "        num_types = 9,\n",
        "        emb_dim = 16,\n",
        "        num_gnn_layers = 5,\n",
        "        simulation_dim = 2\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # IMPORTANT: this is the input dimension of data. It means that the model\n",
        "        # gets data from 2 previouos frames(2*sim_dim) plus the embedding.\n",
        "        # this variable is precomputed here for transparency and used in node_input\n",
        "        # new: hardcoded -> 14 = 7 * 2, for each frame(7) we have pos and vel (1+1)\n",
        "        node_input_dim = 14 + emb_dim\n",
        "        \n",
        "        # classic torch.nn Embedding\n",
        "        self.embedding = Embedding(num_types, emb_dim)\n",
        "\n",
        "        # node inputs and outputs\n",
        "        self.node_input = MLP(node_input_dim, hidden_dim, hidden_dim)\n",
        "        self.node_output = MLP(hidden_dim, simulation_dim, hidden_dim)\n",
        "\n",
        "        self.edge_input = MLP(simulation_dim + 1, hidden_dim, hidden_dim, layer_norm=False)\n",
        "\n",
        "        # initialize gnn layers as InteractionNetwork layers\n",
        "        self.gnns = ModuleList([InteractionNetwork(hidden_dim) for i in range(num_gnn_layers)])\n",
        "\n",
        "        # just save number of layers for later use\n",
        "        self.num_gnns = num_gnn_layers\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
        "\n",
        "    def forward(self, data):\n",
        "        # first we embed the data into features\n",
        "        # SELF NOTE: I was guessing that `data` will have keys `x`, `pos`, `edge_attr` and `edge_index`\n",
        "        \n",
        "        #print(cat((self.embedding(data.x)[:,0,0], data.pos[:,0]), dim=-1).shape)\n",
        "        #node_features = self.node_input(cat((self.embedding(data.x), data.pos), dim=-1))\n",
        "        node_features = cat((self.embedding(data.x), data.pos), dim=-1)\n",
        "        \n",
        "        node_features = self.node_input(node_features)\n",
        "        edge_features = self.edge_input(data.edge_attr)\n",
        "\n",
        "        # then propagate them trough model layers\n",
        "        for gnn in self.gnns:\n",
        "            node_features, edge_features = gnn(x=node_features, edge_feature=edge_features, edge_index=data.edge_index)\n",
        "\n",
        "        # and finally return node positions, in our case: x, y coordinates\n",
        "        node_output = self.node_output(node_features)\n",
        "\n",
        "        return node_output"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and testing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first define the arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = {\n",
        "        'input_dim': 3,\n",
        "        'output_dim': 3,\n",
        "        'hidden_dim': 128,\n",
        "        'layer_norm': True,\n",
        "        'lr': 0.01,\n",
        "        'weight_decay': 5e-3,\n",
        "        'batch_size': 4,\n",
        "        'epochs': 2,\n",
        "        'dropout': 0.5,\n",
        "        'opt': 'adam',\n",
        "        'validate_interval': 1000,\n",
        "        'save_model': True,\n",
        "        'model_path': './model.pt'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch_geometric as pyg\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_onestep(model: torch.nn.Module, data_loader: pyg.data.Dataset, device: torch.device): # type: ignore\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for batch_number, data in enumerate(data_loader):\n",
        "        data = data.to(device)\n",
        "        out = model(data)\n",
        "        loss = F.mse_loss(out, data.y)\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / (batch_number + 1) # reportUnboundVariable: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def train(args: dict[str, Any], model: torch.nn.Module, train_loader, valid_loader):    \n",
        "    # Set the device to GPU if available, otherwise CPU\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "    \n",
        "    # init optimiser\n",
        "    if args['opt'] == 'adam':\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
        "    elif args['opt'] == 'sgd':\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
        "    else:\n",
        "        raise ValueError('Unknown optimizer: {}'.format(args['opt']))\n",
        "    \n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
        "    \n",
        "    # loss function\n",
        "    loss_fn = nn.MSELoss()\n",
        "    \n",
        "    # track the losses to be able to plot the learning curve\n",
        "    train_loss = []\n",
        "    validate_loss = []\n",
        "    \n",
        "    # track the total number of steps\n",
        "    steps = 0\n",
        "    \n",
        "    # main train loop\n",
        "    i = 0\n",
        "    for epoch in range(args['epochs']):\n",
        "        i += 1\n",
        "        print(i)\n",
        "        model.train()\n",
        "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
        "        \n",
        "        # keep track of the total loss and the number of batches\n",
        "        total_loss = 0\n",
        "        batch_count = 0\n",
        "        \n",
        "        for data in progress_bar:\n",
        "            # forward pass\n",
        "            optimizer.zero_grad()\n",
        "            data = data.to(device)\n",
        "            model = model.to(device)\n",
        "            out = model(data)\n",
        "            loss = loss_fn(out, data.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            \n",
        "            # update progress bar\n",
        "            total_loss += loss.item()\n",
        "            batch_count += 1\n",
        "            progress_bar.set_postfix({\"loss\": loss.item(), \"avg_loss\": total_loss / batch_count})\n",
        "            steps += 1\n",
        "            train_loss.append((steps, loss.item()))\n",
        "\n",
        "            # evaluation\n",
        "            if steps % args[\"validate_interval\"] == 0:\n",
        "                model.eval()\n",
        "                loss = validate_onestep(model, valid_loader, device)\n",
        "                validate_loss.append((steps, loss))\n",
        "                tqdm.write(f\"\\nEval: Loss: {validate_loss}\")\n",
        "                model.train()\n",
        "    \n",
        "    if args['save_model']:\n",
        "        torch.save(model.state_dict(), args['model_path'])\n",
        "    \n",
        "    return train_loss, validate_loss"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now for the actual initialization of the data, model and training of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Krasiren\\anaconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0:  34%|███▎      | 1001/2982 [03:13<5:47:32, 10.53s/it, loss=0.515, avg_loss=5.36]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Eval: Loss: [(1000, 8.338850016081894)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0:  67%|██████▋   | 2000/2982 [06:10<3:08:44, 11.53s/it, loss=3.97, avg_loss=7.39] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Eval: Loss: [(1000, 8.338850016081894), (2000, 8.338850020002198)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 2982/2982 [07:35<00:00,  6.55it/s, loss=0.953, avg_loss=6.69] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:   1%|          | 19/2982 [01:21<13:48:36, 16.78s/it, loss=62.2, avg_loss=7.61]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Eval: Loss: [(1000, 8.338850016081894), (2000, 8.338850020002198), (3000, 8.33885002104228)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:  34%|███▍      | 1019/2982 [04:18<8:47:21, 16.12s/it, loss=2.21e+3, avg_loss=9.68] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Eval: Loss: [(1000, 8.338850016081894), (2000, 8.338850020002198), (3000, 8.33885002104228), (4000, 8.338850021122285)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:  68%|██████▊   | 2020/2982 [07:10<2:12:28,  8.26s/it, loss=2.44, avg_loss=7.36]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Eval: Loss: [(1000, 8.338850016081894), (2000, 8.338850020002198), (3000, 8.33885002104228), (4000, 8.338850021122285), (5000, 8.338850020322223)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 2982/2982 [08:40<00:00,  5.73it/s, loss=0.953, avg_loss=6.69] \n"
          ]
        }
      ],
      "source": [
        "# here the actual training takes place\n",
        "train_dataset = OneStepDataset(\"dataset/WaterDropSample/\", \"data/train\") # TODO: init train dataset\n",
        "valid_dataset = OneStepDataset(\"dataset/WaterDropSample/\", \"data/valid\") # TODO: init valid dataset\n",
        "\n",
        "# train_loader = pyg.data.DataLoader(train_dataset, batch_size=args['batch_size'], drop_last=True, shuffle=True, pin_memory=True, num_workers=8)\n",
        "# valid_loader = pyg.data.DataLoader(valid_dataset, batch_size=args['batch_size'], drop_last=True, shuffle=False, pin_memory=True, num_workers=8)\n",
        "\n",
        "train_loader = pyg.data.DataLoader(train_dataset, batch_size=args['batch_size'], drop_last=True, shuffle=False)\n",
        "valid_loader = pyg.data.DataLoader(valid_dataset, batch_size=args['batch_size'], drop_last=True, shuffle=False)\n",
        "\n",
        "#train_loader.dataset.get(0)\n",
        "\n",
        "model = GNS() # TODO: init model to GNS\n",
        "\n",
        "train_loss, validate_loss = train(args, model, train_loader, valid_loader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot the loss curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x1200 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# visualize the loss curve\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.plot(*zip(*train_loss), label=\"train\")\n",
        "plt.plot(*zip(*validate_loss), label=\"valid\")\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0 / 745\n",
            "out.shape: torch.Size([1556, 2])\n",
            "for_video[idx].shape: torch.Size([403, 2])\n"
          ]
        }
      ],
      "source": [
        "from numpy import number\n",
        "import torch\n",
        "import torch_geometric as pyg\n",
        "\n",
        "# read model from the file at args['model_path']\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = GNS()\n",
        "state_dict = torch.load(args['model_path'], map_location=torch.device('cpu') if not torch.cuda.is_available() else torch.device('cuda'))\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "test_dataset = OneStepDataset(\"dataset/WaterDropSample/\", \"data/test\")\n",
        "test_loader = pyg.data.DataLoader(test_dataset, batch_size=args['batch_size'], drop_last=True, shuffle=False)\n",
        "\n",
        "idx = 0\n",
        "for_video = torch.zeros((1000, 403, 2))\n",
        "\n",
        "total_loss = 0\n",
        "number_of_batches = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for batch, data in enumerate(test_loader):\n",
        "        data = data.to(device)\n",
        "        model = model.to(device)\n",
        "        out = model(data)\n",
        "        \n",
        "        total_loss += F.mse_loss(out, data.y).item()\n",
        "        number_of_batches += 1\n",
        "        if batch % 100 == 0:\n",
        "            print(f\"Batch {batch} / {len(test_loader)}\")\n",
        "            \n",
        "        if idx < 1000:\n",
        "            try:\n",
        "                print('out.shape:', out.shape)\n",
        "                print('for_video[idx].shape:', for_video[idx].shape)\n",
        "                for_video[idx] = out\n",
        "                idx += 1\n",
        "            except RuntimeError:\n",
        "                break\n",
        "        if idx == 1000:\n",
        "            break\n",
        "        \n",
        "total_loss /= number_of_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 8750059.0\n",
            "torch.Size([1000, 403, 2])\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test loss: {total_loss}\")\n",
        "print(for_video.shape)\n",
        "torch.save(for_video, \"for_video.pt\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
