{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We used a subset of WaterDrop dataset from Deepmind. The videos only covers the specific case of a water droplet in vacuum, but that is fine with us, as that is exactly what we wanted to model!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting TFRecord to torch tensors\n",
    "\n",
    "Unfortunately, the dataset is not available in a format that is easy to use with PyTorch. We need to convert it to a format that is more suitable for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 17:21:01.773827: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-22 17:21:13.251536: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': array([0]), 'particle_type': array([[5, 0, 0, ..., 0, 0, 5]], dtype=uint8)}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# for example in tf.data.TFRecordDataset(\"./dataset/WaterDropSample/test.tfrecord\").take(1):\n",
    "#     parsed_example = tf.train.Example.FromString(example.numpy())\n",
    "#     print(parsed_example.features.feature)\n",
    "\n",
    "raw_dataset = tf.data.TFRecordDataset(\"./dataset/WaterDropSample/test.tfrecord\")\n",
    "\n",
    "for raw_record in raw_dataset.take(1):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    # print(example)\n",
    "    \n",
    "    result = {}\n",
    "    # example.features.feature is the dictionary\n",
    "    for key, feature in example.features.feature.items():\n",
    "    # The values are the Feature objects which contain a `kind` which contains:\n",
    "    # one of three fields: bytes_list, float_list, int64_list\n",
    "\n",
    "        kind = feature.WhichOneof('kind')\n",
    "        #print(kind)\n",
    "        result[key] = np.array(getattr(feature, kind).value)\n",
    "        #print(result[key])\n",
    "        #print(result[key].dtype.type)\n",
    "\n",
    "        # exmaple: particle_type: bytes_list -> numpy array unit8 (= byte array)\n",
    "        # looks like we don't need conversion of float_list and int64_list types (not proven)\n",
    "        if result[key].dtype.type == np.bytes_:\n",
    "            arr = np.frombuffer(b''.join(result[key]), dtype=np.uint8)\n",
    "            arr = arr.reshape((-1, len(result[key][0])))\n",
    "            result[key] = arr\n",
    "            #print(\"akka\")\n",
    "\n",
    "    print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "1. Apply noise to the training to mitigate error accumulation over long rollouts. We use a simple approach to make the model more robust to noisy inputs: at training we corrupt the input velocities of the model with random-walk noise N (0, $\\sigma_v$ = 0.0003) (adjusting input positions), so the training distribution is closer to the distribution generated during rollouts. \n",
    "2. Normalize all input and target vectors elementwise to zero mean and unit variance, using statistics computed online during training. Preliminary experiments showed that normalization led to faster training, though converged performance was not noticeably improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "MLP is used in a lot of different places throughout the architecture, most notably the encoder and the decoder are both MLPs. We define it as a class to make it easier to use.\n",
    "\n",
    "All MLPs have two hidden layers (with ReLU activations), followed by a nonactivated output layer, each layer with size of 128. All MLPs (except the output decoder) are followed by a LayerNorm layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128, layer_norm=True):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.layer_norm = layer_norm\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        # The rationale behind setting the standard deviation of the normal distribution to 1/sqrt(layer.in_features)\n",
    "        # is to normalize the variance of the layer's inputs and outputs. This helps to prevent the outputs\n",
    "        # from exploding or vanishing during training. The 1/sqrt(layer.in_features) factor is based on the recommendation\n",
    "        # in the paper \"Understanding the difficulty of training deep feedforward neural networks\" by Glorot and Bengio (2010).\n",
    "        self.layer1.weight.data.normal_(0, 1 / torch.sqrt(self.layer1.in_features))\n",
    "        # Setting the bias to 0 allows the network to learn the appropriate bias values during training.\n",
    "        self.layer1.bias.data.fill_(0)\n",
    "        # The same reasoning applies to the other layers.\n",
    "        self.layer2.weight.data.normal_(0, 1 / torch.sqrt(self.layer2.in_features))\n",
    "        self.layer2.bias.data.fill_(0)\n",
    "        self.layer3.weight.data.normal_(0, 1 / torch.sqrt(self.layer3.in_features))\n",
    "        self.layer3.bias.data.fill_(0)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer2(x)\n",
    "        if self.layer_norm:\n",
    "            x = nn.LayerNorm(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "        'input_dim': 3,\n",
    "        'output_dim': 3,\n",
    "        'hidden_dim': 128,\n",
    "        'layer_norm': True,\n",
    "        'lr': 0.01,\n",
    "        'weight_decay': 5e-3,\n",
    "        'batch_size': 2,\n",
    "        'epochs': 1,\n",
    "        'dropout': 0.5,\n",
    "        'opt': 'adam',\n",
    "        'validate_interval': 1000,\n",
    "        'save_model': True,\n",
    "        'model_path': './model.pt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric as pyg\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_onestep(model: torch.nn.Module, data_loader: pyg.data.Dataset, device: torch.device): # type: ignore\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for batch_number, data in enumerate(data_loader):\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        loss = F.mse_loss(out, data.y)\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / (batch_number + 1) # reportUnboundVariable: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(args: dict[str, Any], model: torch.nn.Module, train_loader, valid_loader):\n",
    "    \n",
    "    # Set the device to GPU if available, otherwise CPU\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    # init optimiser\n",
    "    if args['opt'] == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "    elif args['opt'] == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "    else:\n",
    "        raise ValueError('Unknown optimizer: {}'.format(args['opt']))\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
    "    \n",
    "    # loss function\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # track the losses to be able to plot the learning curve\n",
    "    train_loss = []\n",
    "    validate_loss = []\n",
    "    \n",
    "    # track the total number of steps\n",
    "    steps = 0\n",
    "    \n",
    "    # main train loop\n",
    "    for epoch in range(args['epochs']):\n",
    "        model.train()\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "        \n",
    "        # keep track of the total loss and the number of batches\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for data in progress_bar:\n",
    "            # forward pass\n",
    "            optimizer.zero_grad()\n",
    "            data = data.cuda()\n",
    "            out = model(data)\n",
    "            loss = loss_fn(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # update progress bar\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            progress_bar.set_postfix({\"loss\": loss.item(), \"avg_loss\": total_loss / batch_count})\n",
    "            steps += 1\n",
    "            train_loss.append((steps, loss.item()))\n",
    "\n",
    "            # evaluation\n",
    "            if steps % args[\"validate_interval\"] == 0:\n",
    "                model.eval()\n",
    "                loss = validate_onestep(model, valid_loader, device)\n",
    "                validate_loss.append((steps, validate_loss))\n",
    "                tqdm.write(f\"\\nEval: Loss: {validate_loss}\")\n",
    "                model.train()\n",
    "    \n",
    "    if args['save_model']:\n",
    "        torch.save(model.state_dict(), args['model_path'])\n",
    "    \n",
    "    return train_loss, validate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7362, 0.1503],\n",
      "        [0.6930, 0.1384],\n",
      "        [0.6923, 0.1360],\n",
      "        ...,\n",
      "        [0.1229, 0.1704],\n",
      "        [0.3545, 0.1309],\n",
      "        [0.8870, 0.3163]])\n"
     ]
    }
   ],
   "source": [
    "# read from file\n",
    "labels = torch.load('./data/labels.pt')\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7411, 0.1506],\n",
      "        [0.6980, 0.1388],\n",
      "        [0.6973, 0.1364],\n",
      "        ...,\n",
      "        [0.1240, 0.1606],\n",
      "        [0.3643, 0.1316],\n",
      "        [0.8871, 0.3275]])\n"
     ]
    }
   ],
   "source": [
    "positions = torch.load('./data/positions.pt')\n",
    "print(positions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
